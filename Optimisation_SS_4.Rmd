---
title: "Optimisation: Self study 4 -- Least squares"
output: html_document
---
\usepackage{amsmath}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(matrixcalc)
```
Modellen der arbejdes med er
\begin{align*}
\phi(\boldsymbol x;\boldsymbol t).
\end{align*}

I generelle least square problemer, vil man finde parametre $\boldsymbol x$ sådan at objektfunktionen

\begin{align}
  f(\boldsymbol x) = \frac{1}{2}\sum_{j=1}^mr_j(\boldsymbol x)^2 \tag{10.1}
\end{align}

minimeres. Hvor

\begin{align*}
  r_j(\boldsymbol x) = \phi(\boldsymbol x;t_j)-y_j.
\end{align*}

Objektfunktion skrevet på vektorform er 

\begin{align}
  f(\boldsymbol x) = \frac{1}{2}||\boldsymbol{r}(\boldsymbol x)||^2,
  \label{numeriskresid}
\end{align}
hvor $\boldsymbol{r}(\boldsymbol x) = (r_1(\boldsymbol x),\ldots,r_m(\boldsymbol x))^\top$.

\begin{align*}
J(\boldsymbol x) = 
\begin{bmatrix} 
  \nabla r_1(\boldsymbol x)^\top\\
  \nabla r_2(\boldsymbol x)^\top\\
  \vdots\\
  \nabla r_m(\boldsymbol x)^\top
\end{bmatrix}
=
\begin{bmatrix}
  \frac{\partial r_1}{\partial x_1} & \frac{\partial r_1}{\partial x_2} & \cdots & \frac{\partial r_1}{\partial x_m} \\
  \frac{\partial r_2}{\partial x_1} & \frac{\partial r_2}{\partial x_2} & \cdots & \frac{\partial r_2}{\partial x_m} \\
  \vdots & \vdots & \ddots & \vdots \\
  \frac{\partial r_m}{\partial x_1} & \frac{\partial r_m}{\partial x_2} & \cdots & \frac{\partial r_m}{\partial x_m}
\end{bmatrix}\tag{10.3}
\end{align*}
Gradienten og Hessematricen for $f$ kan udtrykkes som
\begin{align*}
\nabla f(x) &= \sum_{j = 1}^{m}r_{j}(x)\nabla r_{j}(x)=J(x)^\top r(x)\tag{10.4}\\
\nabla ^ { 2 } f ( x ) &= \sum _ { j = 1 } ^ { m } \nabla r _ { j } ( x ) \nabla r _ { j } ( x ) ^ \top + \sum _ { j = 1 } ^ { m } r _ { j } ( x ) \nabla ^ { 2 } r _ { j } ( x )\\
&= J ( x )^\top J ( x ) + \sum _ { j = 1 } ^ { m } r _ { j } ( x ) \nabla ^ { 2 } r _ { j } ( x )\tag{10.5}
\end{align*}
***

# Exercise 1: OLS

Explain and show different ways to solve an OLS problem (e.g. `cars` dataset) using matrix factorisations. 

Hvad er et OLS problem?

***

I tilfældet af at $\phi(\boldsymbol x;\boldsymbol t)$ er en lineær funktion af $\boldsymbol x$, vil det medføre at residualerne $r_j(\boldsymbol x)$ også er lineære. Så kaldes det problem man gerne vil løse et 'Linear least square problem'. Så kan residualerne opstilles $\boldsymbol r(\boldsymbol x) = J\boldsymbol x - \boldsymbol y$ og objektfunktionen \eqref{numeriskresid} kan skrives
\begin{align}
  f(\boldsymbol x) = \frac{1}{2}||J\boldsymbol x - \boldsymbol y|| \tag{10.13}.
\end{align}
Gradienten og Hessematricen er nu på formen
\begin{align*}
  \nabla f(\boldsymbol x) &= J^\top (J\boldsymbol x - \boldsymbol y)\\
  \nabla^2 f(\boldsymbol x) &= J^\top J.
\end{align*}
Da hessematricen, $\nabla^2 f(\boldsymbol x)=  J^\top J$, er positiv semidefinit, er $f$ konveks. Jf. Sætning 2.5 gælder der så at ethvert punkt $\boldsymbol{x}^*$ der opfylder $\nabla f(\boldsymbol{x}^*)=0$ er en 'global minimiser' af $f$. Derfor må $\boldsymbol{x}^*$ opfylde
\begin{align} 
  0 = \nabla f(\boldsymbol{x}^*) &= J^\top (J\boldsymbol{x}^* - \boldsymbol y)\nonumber\\
  &\Updownarrow\nonumber\\
  J^\top J\boldsymbol{x}^*&=J^\top \boldsymbol y. \tag{10.14}
\end{align}
Ligning (10.14) er 'normalligningerne'.



Vil finde en model på formen "$\text{dist} = x_0 + x_1 \cdot \text{speed}$". Det vil sige vores designmatrix er som følgende
```{r}
designmatrix <- matrix(1, nrow = length(cars$speed), ncol = 2)
designmatrix[,2] <- cars$speed
```
Vil bestemme J (jacobi-matricen), den er givet ved 
```{r}
J <- designmatrix
```
Skal nu finde de $x$-værdier (normalt vores betaer) som gør at residualerne er mindst. Et minimum for x'erne skal jf. (10.14) opfylde $J^\top J \boldsymbol x=J^\top\cdot dist$. For at løse dette benyttes Cholesky-metoden, hvor $A=J^\top J$ og $\boldsymbol b=J^\top \cdot dist$, da fås at $A\boldsymbol x=\boldsymbol b$:
```{r}
A <- t(J) %*% J
b <- t(J) %*% cars$dist
```
Der skal nu løses $A\boldsymbol x=\boldsymbol b$. Dette gøres ved brug af Cholesky-metode. Først implementeres der en algoritme der udfører en Cholesky faktorisering.

```{r}
CholAlg <- function(A){
if(is.positive.definite(A)){ # Tjekker dog ikke om den der symetrisk (kommer fra matrixcalc)
  n <- dim(A)[1]
  L <- matrix(0, nrow = n, ncol = n)
  for (i in 1:n) {
    L[i,i] <- sqrt(A[i,i])
    if(i == n) next
    for (j in (i+1):n){
      L[j,i] <- A[j,i] / L[i,i]
      if(i == j) next
      for (k in (i+1):j){
        A[j,k] <- A[j,k] - L[j,i] * L[k,i]
      }
    }
  }
  return(t(L))
}
else {stop("Inputmatricen er ikke positiv definit")}
}
```

Nu kan $L$ findes, sådan at $A=LL^\top$. (Bemærk: $CholAlg(A) = L^\top$ som er en øvre trekantsmatrix, ligesom R's chol funktion.)
```{r}
L <- t(CholAlg(A))
```
Finder $\boldsymbol z$ ud fra $L\boldsymbol z=\boldsymbol b$. Da $L$ er en nedre trekantsmatrix, vil der ved brug af forwardsolving skulle løses $n$ lineære ligninger med én ubekendt.
```{r}
z <- forwardsolve(L,b)
```
Finder nu $\boldsymbol x$ udfra $L^\top \boldsymbol x=\boldsymbol z$. Da $L^\top$ er en nedre trekantsmatrix, vil der ved brug af backwardsolving skulle løses $n$ lineære ligninger med én ubekendt.
```{r}
x_star <- backsolve(t(L), z)
```
Nu haves de $x$-værdier (betaer) som minimerer residualerne ($J\boldsymbol x-dist$)
```{r}
x_star
```
Disse $x$-værdier kan sammenlignes med de værdier som fås ved brug *lm*-funktionen
```{r}
lm_cars <- lm(cars$dist ~ cars$speed, data = cars)

x_lm <- coefficients(lm_cars)

x_lm
```
**QR-faktorisering**

_QR-faktoriseringen_ af *J* er givet ved
\begin{align*}
    JP & = QR
         = Q\begin{bmatrix}
            R\\
            0
            \end{bmatrix}
         = \begin{bmatrix}
         Q_1  Q_2
           \end{bmatrix}  
           \begin{bmatrix}
             R\\
             0
           \end{bmatrix}
         = Q_1R\tag{10.17}, 
\end{align*}
hvor

* $P$ er en $n \times n$ permutationsmatrix.
* $Q$ er en $m \times m$ ortogonalmatrix.
* $Q_1$ er de første $n$ søjler i $Q$.
* $R$ er en $n \times n$ øvre trekantsmatrix.

Der haves at
\begin{align*}
\|J\boldsymbol{x}-\boldsymbol{y}\| 
     &=  \left\|R(P^\top\boldsymbol{x})-Q_1^\top\boldsymbol{y}\right\|^2 + \left\|Q_2^\top\boldsymbol{y}\right\|^2.\tag{10.18}
\end{align*}
Da kan $\|J\boldsymbol{x}-\boldsymbol{y}\|$ minimeres ved at tvinge første udtryk mod nul, og altså opnås
\begin{align*}
    \boldsymbol{x}^* = PR^{-1} Q_1^\top \boldsymbol{y}.
\end{align*}
Dette gøres i praksis ved at løse $Rz=Q_1^\top\boldsymbol{y}$, for derefter at permutere komponenterne af $\boldsymbol{z}$, for at opnå $\boldsymbol{x}^*=P\boldsymbol{z}$.
```{r}
qr.coef(qr(J),cars$dist)
```
**SVD-Metoden**

_SVD-faktoriseringen_ for $J$ har er givet ved
\begin{align*}
    J   = U \begin{bmatrix} S \\ 0 \end{bmatrix} V^\top, 
        = \begin{bmatrix} U_1 & U_2 \end{bmatrix} \begin{bmatrix} S \\ 0 \end{bmatrix} V^\top,
        = U_1 S V^\top\tag{10.19},
\end{align*}
hvor

* $U$ er en $m \times m$ matrix.
* $U_1$ består af de første $n$ søjler af $U$, og $U_2$ består af de sidste $m - n$ søjler.
* $V$ er en $n \times n$ ortogonalmatrix.
* $S$ er en $n \times n$ diagonalmatrix med de singulære værdier, $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_n > 0$. 

Der haves at
\begin{align*}
    \| J \boldsymbol x - \boldsymbol y \|^2
    = \| S(V^\top \boldsymbol x) - U_1^\top \boldsymbol y \|^2 + \| U_2^\top \boldsymbol y \|^2, \tag{10.20}
\end{align*}
som kan minimeres ved at tvinge første led mod nul. Da vil minimizeren være givet ved
\begin{align*}
    \boldsymbol x^* = V S^{-1} U_1^\top \boldsymbol y.
\end{align*}
Ved at betegne den $i$'te søjle i $U$ og $V$ som henholdsvis $\boldsymbol{u}_i, \in \mathbb R^m$ og $\boldsymbol v_i \in \mathbb R^n$, fås
\begin{align*}
    \boldsymbol{x}^* = \sum_{i = 1}^n \frac{\boldsymbol{u}_i ^\top \boldsymbol{y}}{\sigma_i}\boldsymbol{v}_i. \tag{10.21}
\end{align*}

```{r}
svdfact <- svd(J)
#svdfact$u %*% diag(svdfact$d) %*% t(svdfact$v)


svdfact$v %*% diag(1/svdfact$d) %*% t(svdfact$u) %*% cars$dist
```

***

# Exercise 2: NLS

In this exercise the `USPop` data from the `car` package is used (`data(USPop, package = "car")`).

Analyse this data as an NLS problem. Include discussion of starting values (see "Nonlinear Regression and Nonlinear Least Squares in R" by John Fox & Sanford Weisberg, available at Moodle).

Discuss which of Gauss-Newton (`nls()`) and Levenberg-Marquardt (`minpack.lm` library) that are more fragile to starting values.

Can you solve this optimisation problem in other ways than by Gauss-Newton/Levenberg-Marquardt?

***