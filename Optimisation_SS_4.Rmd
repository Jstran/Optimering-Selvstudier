---
title: 'Optimisation: Self study 4 -- Least squares'
output:
  html_document: default
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(matrixcalc)
library(car)
library(minpack.lm)
```
Modellen der arbejdes med er
\begin{align*}
\phi(\boldsymbol x;\boldsymbol t).
\end{align*}

I generelle least square problemer, vil man finde parametre $\boldsymbol x$ sådan at objektfunktionen

\begin{align}
  f(\boldsymbol x) = \frac{1}{2}\sum_{j=1}^mr_j(\boldsymbol x)^2 \tag{10.1}
\end{align}

minimeres. Hvor

\begin{align*}
  r_j(\boldsymbol x) = \phi(\boldsymbol x;t_j)-y_j.
\end{align*}

Objektfunktion skrevet på vektorform er 

\begin{align}
  f(\boldsymbol x) = \frac{1}{2}||\boldsymbol{r}(\boldsymbol x)||^2,
  \label{numeriskresid}
\end{align}
hvor $\boldsymbol{r}(\boldsymbol x) = (r_1(\boldsymbol x),\ldots,r_m(\boldsymbol x))^\top$.
Jacobimatrien for $f(x)$ er givet ved
\begin{align*}
J(\boldsymbol x) = 
\begin{bmatrix} 
  \nabla r_1(\boldsymbol x)^\top\\
  \nabla r_2(\boldsymbol x)^\top\\
  \vdots\\
  \nabla r_m(\boldsymbol x)^\top
\end{bmatrix}
=
\begin{bmatrix}
  \frac{\partial r_1}{\partial x_1} & \frac{\partial r_1}{\partial x_2} & \cdots & \frac{\partial r_1}{\partial x_m} \\
  \frac{\partial r_2}{\partial x_1} & \frac{\partial r_2}{\partial x_2} & \cdots & \frac{\partial r_2}{\partial x_m} \\
  \vdots & \vdots & \ddots & \vdots \\
  \frac{\partial r_m}{\partial x_1} & \frac{\partial r_m}{\partial x_2} & \cdots & \frac{\partial r_m}{\partial x_m}
\end{bmatrix}\tag{10.3}
\end{align*}
Gradienten og Hessematricen for $f$ kan udtrykkes som
\begin{align*}
\nabla f(x) &= \sum_{j = 1}^{m}r_{j}(x)\nabla r_{j}(x)=J(x)^\top r(x)\tag{10.4}\\
\nabla ^ { 2 } f ( x ) &= \sum _ { j = 1 } ^ { m } \nabla r _ { j } ( x ) \nabla r _ { j } ( x ) ^ \top + \sum _ { j = 1 } ^ { m } r _ { j } ( x ) \nabla ^ { 2 } r _ { j } ( x )\\
&= J ( x )^\top J ( x ) + \sum _ { j = 1 } ^ { m } r _ { j } ( x ) \nabla ^ { 2 } r _ { j } ( x )\tag{10.5}
\end{align*}
***

# Exercise 1: OLS

Explain and show different ways to solve an OLS problem (e.g. `cars` dataset) using matrix factorisations. 

Hvad er et OLS problem?

***

I tilfældet af at $\phi(\boldsymbol x;\boldsymbol t)$ er en lineær funktion af $\boldsymbol x$, vil det medføre at residualerne $r_j(\boldsymbol x)$ også er lineære. Så kaldes det problem man gerne vil løse et 'Linear least square problem'. Så kan residualerne opstilles $\boldsymbol r(\boldsymbol x) = J\boldsymbol x - \boldsymbol y$, hvor $J$ er designmatricen og objektfunktionen \eqref{numeriskresid} kan skrives
\begin{align}
  f(\boldsymbol x) = \frac{1}{2}||J\boldsymbol x - \boldsymbol y||^2 \tag{10.13}.
\end{align}
Gradienten og Hessematricen er nu på formen
\begin{align*}
  \nabla f(\boldsymbol x) &= J^\top (J\boldsymbol x - \boldsymbol y)\\
  \nabla^2 f(\boldsymbol x) &= J^\top J.
\end{align*}
Da hessematricen, $\nabla^2 f(\boldsymbol x)=  J^\top J$, er positiv semidefinit, er $f$ konveks. Jf. Sætning 2.5 gælder der så at ethvert punkt $\boldsymbol{x}^*$ der opfylder $\nabla f(\boldsymbol{x}^*)=0$ er en 'global minimiser' af $f$. Derfor må $\boldsymbol{x}^*$ opfylde
\begin{align} 
  0 = \nabla f(\boldsymbol{x}^*) &= J^\top (J\boldsymbol{x}^* - \boldsymbol y)\nonumber\\
  &\Updownarrow\nonumber\\
  J^\top J\boldsymbol{x}^*&=J^\top \boldsymbol y. \tag{10.14}
\end{align}
Ligning (10.14) er 'normalligningerne'.



Vil finde en model på formen "$\text{dist} = x_0 + x_1 \cdot \text{speed}$". Det vil sige vores designmatrix er som følgende
```{r}
designmatrix <- matrix(1, nrow = length(cars$speed), ncol = 2)
designmatrix[,2] <- cars$speed
y <- cars$dist
```
Vil bestemme J, den er givet ved 
```{r}
J <- designmatrix
```
Skal nu finde de $x$-værdier som gør at residualerne er mindst. Et minimum for x'erne skal jf. (10.14) opfylde $J^\top J \boldsymbol x=J^\top\cdot dist$. Dette kan gøres ved brug af forskellige faktoriseringsmetoder.

*Cholesky*

For at bestemme $x^*$ benyttes Cholesky-metoden, hvor $A = J^\top J$ og $\boldsymbol b = J^\top \cdot dist$, da fås at $A\boldsymbol x=\boldsymbol b$:
```{r}
A <- t(J) %*% J
b <- t(J) %*% y
```
Der skal nu løses $A\boldsymbol x=\boldsymbol b$. Dette gøres ved brug af Cholesky-metode. Først implementeres der en algoritme der udfører en Cholesky faktorisering.

```{r}
CholAlg <- function(A){
if(is.positive.definite(A)){ # Tjekker dog ikke om den der symetrisk 
  n <- dim(A)[1]
  L <- matrix(0, nrow = n, ncol = n)
  for (i in 1:n) {
    L[i,i] <- sqrt(A[i,i])
    if(i == n) next
    for (j in (i+1):n){
      L[j,i] <- A[j,i] / L[i,i]
      if(i == j) next
      for (k in (i+1):j){
        A[j,k] <- A[j,k] - L[j,i] * L[k,i]
      }
    }
  }
  return(t(L))
}
else {stop("Inputmatricen er ikke positiv definit")}
}
```

Nu kan $L$ findes, sådan at $A = LL^\top$. (Bemærk: $CholAlg(A) = L^\top$ som er en øvre trekantsmatrix, ligesom R's chol funktion.)
```{r}
L <- t(CholAlg(A))
```
\[
LL^\top x = b
\]
Finder $\boldsymbol z$ ud fra $L\boldsymbol z=\boldsymbol b$. Da $L$ er en nedre trekantsmatrix, vil der ved brug af forwardsolving skulle løses $n$ lineære ligninger med én ubekendt.
```{r}
z <- forwardsolve(L,b)
```
Finder nu $\boldsymbol x$ udfra $L^\top \boldsymbol x=\boldsymbol z$. Da $L^\top$ er en øvre trekantsmatrix, vil der ved brug af backwardsolving skulle løses $n$ lineære ligninger med én ubekendt.
```{r}
x_chol <- backsolve(t(L), z)
```
Nu haves de $x$-værdier (betaer) som minimerer residualerne ($J\boldsymbol x-dist$)
```{r}
x_chol
```
Disse $x$-værdier kan sammenlignes med de værdier som fås ved brug *lm*-funktionen
```{r}
lm_cars <- lm(cars$dist ~ cars$speed, data = cars)

x_lm <- coefficients(lm_cars)

x_lm
```
**QR-faktorisering**

_QR-faktoriseringen_ af *J* er givet ved
\begin{align*}
    JP & = QR
         = Q\begin{bmatrix}
            R\\
            0
            \end{bmatrix}
         = \begin{bmatrix}
         Q_1  Q_2
           \end{bmatrix}  
           \begin{bmatrix}
             R\\
             0
           \end{bmatrix}
         = Q_1R\tag{10.17}, 
\end{align*}
hvor

* $P$ er en $n \times n$ permutationsmatrix.
* $Q$ er en $m \times m$ ortogonalmatrix.
* $Q_1$ er de første $n$ søjler i $Q$.
* $R$ er en $n \times n$ øvre trekantsmatrix.

Der haves at
\begin{align*}
\|J\boldsymbol{x}-\boldsymbol{y}\|^2 
     &=  \left\|R(P^\top\boldsymbol{x})-Q_1^\top\boldsymbol{y}\right\|^2 + \left\|Q_2^\top\boldsymbol{y}\right\|^2.\tag{10.18}
\end{align*}
Da kan $\|J\boldsymbol{x}-\boldsymbol{y}\|^2$ minimeres ved at tvinge første udtryk mod nul, og altså opnås
\begin{align*}
    \boldsymbol{x}^* = PR^{-1} Q_1^\top \boldsymbol{y}.
\end{align*}
\begin{align*}
  z = R^{-1} Q_1^\top \boldsymbol{y}.
\end{align*}
Dette gøres i praksis ved at løse $Rz=Q_1^\top\boldsymbol{y}$, for derefter at permutere komponenterne af $\boldsymbol{z}$, for at opnå $\boldsymbol{x}^*=P\boldsymbol{z}$.
```{r}
# Definerer en funktion til QR-faktorisering på baggrund af R's implementerede QR funktion.
QRfun <- function(A){
  QR <- qr(A)
  Q1 <- qr.Q(QR)
  R <- qr.R(QR)
  return(list(Q1 = Q1 , R = R))
}

# Finder QR faktoriseringen for J.
QR_cars <- QRfun(J)

# Minimizeren bestemmes.
z <- backsolve(QR_cars$R, t(QR_cars$Q1) %*% y)
P <- diag(dim(J)[2])
x_qr <- P %*% z ; x_qr
```

**SVD-Metoden**

_SVD-faktoriseringen_ for $J$ har er givet ved
\begin{align*}
    J   = U \begin{bmatrix} S \\ 0 \end{bmatrix} V^\top, 
        = \begin{bmatrix} U_1 & U_2 \end{bmatrix} \begin{bmatrix} S \\ 0 \end{bmatrix} V^\top,
        = U_1 S V^\top\tag{10.19},
\end{align*}
hvor

* $U$ er en $m \times m$ matrix.
* $U_1$ består af de første $n$ søjler af $U$, og $U_2$ består af de sidste $m - n$ søjler.
* $V$ er en $n \times n$ ortogonalmatrix.
* $S$ er en $n \times n$ diagonalmatrix med de singulære værdier, $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_n > 0$. 

Der haves at
\begin{align*}
    \| J \boldsymbol x - \boldsymbol y \|^2
    = \| S(V^\top \boldsymbol x) - U_1^\top \boldsymbol y \|^2 + \| U_2^\top \boldsymbol y \|^2, \tag{10.20}
\end{align*}
som kan minimeres ved at tvinge første led mod nul. Da vil minimizeren være givet ved
\begin{align*}
    \boldsymbol x^* = V S^{-1} U_1^\top \boldsymbol y.
\end{align*}
Ved at betegne den $i$'te søjle i $U$ og $V$ som henholdsvis $\boldsymbol{u}_i, \in \mathbb R^m$ og $\boldsymbol v_i \in \mathbb R^n$, fås
\begin{align*}
    \boldsymbol{x}^* = \sum_{i = 1}^n \frac{\boldsymbol{u}_i ^\top \boldsymbol{y}}{\sigma_i}\boldsymbol{v}_i. \tag{10.21}
\end{align*}

```{r}
# SVD faktorisering ved brug af R's indbyggede funktion.
svdfact <- svd(J)

V <- svdfact$v 
S_inv <- diag(1/svdfact$d) 
U1 <- svdfact$u

# Minimizeren bestemmes.
x_svd <- V %*% S_inv %*%  t(U1) %*% y; x_svd
```

Det ses at alle de tre faktoriseringsmetoder giver det samme resultat som *lm* funktionen gør.
```{r}
data.frame(Cholesky = x_chol , QR = x_qr , SVD = x_svd , LM = x_lm)
```


***

# Exercise 2: NLS

In this exercise the `USPop` data from the `car` package is used (`data(USPop, package = "car")`).

Analyse this data as an NLS problem. Include discussion of starting values (see "Nonlinear Regression and Nonlinear Least Squares in R" by John Fox & Sanford Weisberg, available at Moodle).

Discuss which of Gauss-Newton (`nls()`) and Levenberg-Marquardt (`minpack.lm` library) that are more fragile to starting values.

Can you solve this optimisation problem in other ways than by Gauss-Newton/Levenberg-Marquardt?

***


Der ønskes at lave regression på USPop.
```{r}
# Plotter USPop datasæt med lm regressionslinje
plot(population ~ year, data=USPop, main="(a)")
abline(lm(population ~ year, data=USPop))
```

Det ses at hvis der laves regression med en lineær model, er dette ikke en præcis prædiktor. Vi betragter derfor i stedet en logistisk vækstmodel på formen
$$y = \frac{\theta_1}{1+\exp{(-(\theta_2+\theta_3x))}} + \varepsilon.$$

I modsætning til lineære least-square algortimer, kræver de fleste ikke-lineære least-square algortimer specifikationer af **startværdier** for parametrene $\theta_1, \theta_2$ og $\theta_3$ i den logistiske vægtsmodel
\begin{align*}
     y &\approx \frac{\theta_1}{1+\exp{(-(\theta_2+\theta_3x))}}\\
     &\Updownarrow\\
    \frac{y}{\theta_1} &\approx \frac{1}{1+\exp{(-(\theta_2+\theta_3x))}}\\
    &\Updownarrow\\
    \log\left(\frac{y/\theta_1}{1-y/\theta_1}\right) &\approx \theta_2+\theta_3x
\end{align*}
Hvis der vælges en startværdi for $\theta_1$ kan $\theta_2$ og $\theta_3$ bestemmes ud fra OLS. 
```{r}
# Funktion til at bestemme startværdier.
self_start <- function(y, x){
  theta1 <- max(y) + 1
  
  # Bruger lm til at estimere theta2 og theta3 ud fra.
  mod <- summary(lm(logit(y/theta1) ~ x)) 

  theta2 <- mod$coefficients[1,1]
  theta3 <- mod$coefficients[2,1]

  # De estimerede thetaer.
  start_theta <- list(theta1 = theta1, theta2 = theta2 , theta3 = theta3) 

  return(start_theta)
}

# Bruger self_start på USPop.
y <- USPop$population
x <- USPop$year

start_theta <- self_start(y, x); start_theta

```
**Gauss-Newton metoden**

Kan ses som en modificeret Newton metode med line search. Gauss-Newton for ikke-lineære least square problemer bygger på at approksimationen af hessematricen for objektfunktionen er givet ved

\begin{align*}
\nabla^2 f_k  \approx J_k^\top J_k.
\end{align*}

Dette meføre at man i Gauss-Newton gerne løse 

\begin{align*}
J_k^\top J_k \boldsymbol{p}_k^{GN} = -J_k\boldsymbold{r}_k,
\end{align*}
for at opnå søgningsretningen $\boldsymbol{p}_k^{GN}$

Ud fra ligheden mellem normalligningen fra det lineære least square problem og ovenstående Gauss-Newton ligning haves fordelen, at $p_k^{GN}$ kan løses som det lineære least-square problem
\begin{align*}
\min_p \frac{1}{2} \|J_k \boldsymbold p + \boldsymbol{r}_k\|^2.
\end{align*}

Anvender **Gauss-Newton** og **Levenberg-Marquardt** på USPop datamængden.
```{R}
# Gauss-Newton.
pop.mod.GN <- nls(population ~ theta1/(1 + exp(-(theta2 + theta3*year))), 
                  start=start_theta, data=USPop)

summary(pop.mod.GN)

# Levenberg-Marquardt.
pop.mod.LM <- nlsLM(population ~ theta1/(1 + exp(-(theta2 + theta3*year))), 
                    start=start_theta, data=USPop)

summary(pop.mod.LM)
```
Vælges f.eks. $\theta_1 = 50$ som startværdi, kan *GN* ikke konverge, hvorimod *LM* kan. 