---
title: 'Optimisation: Self study 3 -- Quasi-Newton DFO'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
printf <- function(...) cat(sprintf(...))
printfln <- function(...) cat(sprintf(...), "\n")
library(microbenchmark)
```

# Exercise 1: BFGS, DFP and SR1

***
Implement BFGS, DFP and SR1 (as line search).

***
_Line Search iteration_\
\[
x_{k+1}=x_k+\alpha_kp_k,
\]
hvor $\alpha_k$ er skridtlængden som skal opfylde _Wolfe conditions_, og søgningsretningen $p_k$ er givet ved\
\[
    p_k=-B_k^{-1}\nabla f(x_k).
\]

_Sekantligning_\
\[
    B_{k+1}s_k=y_k,
\]
hvor 
\[
    s_k = x_{k+1} - x_k = \alpha_k p_k, \qquad 
    y_k = \nabla f_{k+1} - \nabla f_k.
\]

_Curvature betingelsen_\
\[
    s_k^\top y_k > 0.
\]

Betragter forskellige _Quasi-Newton metoder_.

**DFP** opdateringsformel\
\[
  B_{k+1} = (I -\rho_k y_k s_k^\top) B_k (I -\rho_k s_k y_k^\top) + \rho_k y_k y_k^\top.\tag{DFP}
\]
Inverteret formel\
\[
     H_{k+1} = H_k - \frac{H_k y_k y_k^\top H_k}{y_k^\top H_k y_k} + \frac{s_k s_k^\top}{y_k^\top}. \tag{DFP}
\]

**BFGS Metoden** opdaterings formel\
\[
    H_{k+1} = (I -\rho_k y_k s_k^\top) H_k (I -\rho_k s_k y_k^\top) + \rho_k y_k y_k^\top.\tag{BFGS}
\]
Det er muligt at opdatere BFGS ved efter første skridt er udført at sætte
\[ 
   H_0 = \frac{y_k^\top s_k}{y_k^\top y_k} I_n
\]
før man opdatere til $H_1$. Dette skulle gøre størrelsen af den $H_0$ tættere på den sande inverterede hessematrice. 
 
**SR1** opdateringsformel\
\[
    H_{k+1} = H_k + \frac{(s_k - H_ky_k) (s_k-H_ky_k)^\top}{(s_k - H_ky_k)^\top y_k}\tag{SR1}
\]

BFGS, den opdaterede BFGS og DFP implementeres.
```{r}
# Implementation af backtracking
backtracking_line_search <- function(alpha, c, rho, x_k, p_k, g_k, f) {
  alpha_k <- alpha
	repeat { 
		lhs <- f(x_k + alpha_k * p_k)
		rhs <- f(x_k) + c * alpha_k * g_k %*% p_k
		if (lhs <= rhs) break
		alpha_k <- rho * alpha_k
	}
	return(alpha_k)
}

# Implementation af BFGS
BFGS <- function(f, g, x_k, alpha, c, rho, tol, k_max){
  k <- 0
  n <- length(x_k)
  H_k <- I_n <- diag(n)
  g_k <- tol + 1
  while (norm(g_k, type="2") > tol & k < k_max) {
    g_k <- g(x_k)
    p_k <- -  H_k %*% g_k
    alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
    x_k <- x_k + alpha_k * p_k
    s_k <- alpha_k * p_k
    y_k <- g(x_k) - g_k
    rho_k <- 1 / (t(y_k) %*% s_k)[1]
    H_k <- (I_n - rho_k*s_k%*%t(y_k))%*%H_k%*%(I_n - rho_k*y_k%*%t(s_k)) +
      rho_k*s_k%*%t(s_k)
    k <- k + 1
  }
  sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k , inv_hess = H_k)
  return(sol)
}

# Implementation af BFGS opdatering
BFGS_620 <- function(f, g, x_k, alpha, c, rho, tol, k_max){
  n <- length(x_k)
  H_k <- I_n <- diag(n)
  g_k <- g(x_k)
    p_k <- -  H_k %*% g_k
    alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
    x_k <- x_k + alpha_k * p_k
    s_k <- alpha_k * p_k
    y_k <- g(x_k) - g_k
    H_k <- ((t(y_k) %*% s_k)[1]) / ((t(y_k) %*% y_k)[1]) * I_n
    k <- 1
  while (norm(g_k, type="2") > tol & k < k_max) {
    g_k <- g(x_k)
    p_k <- -  H_k %*% g_k
    alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
    x_k <- x_k + alpha_k * p_k
    s_k <- alpha_k * p_k
    y_k <- g(x_k) - g_k
    rho_k <- 1 / (t(y_k) %*% s_k)[1]
    H_k <- (I_n - rho_k*s_k%*%t(y_k))%*%H_k%*%(I_n - rho_k*y_k%*%t(s_k)) +
      rho_k*s_k%*%t(s_k)
    k <- k + 1
  }
sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k , inv_hess = H_k)
  return(sol)
}

# Implementation af DFP
DFP <- function(f, g, x_k, alpha, c, rho, tol, k_max){
  k <- 0
  n <- length(x_k)
  H_k <- I_n <- diag(n)
  g_k <- tol + 1
  while (norm(g_k, type="2") > tol & k < k_max) {
    g_k <- g(x_k)
    p_k <- -  H_k %*% g_k
    alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
   x_k <- x_k + alpha_k * p_k
    s_k <- alpha_k * p_k
    y_k <- g(x_k) - g_k
    H_k <- H_k - (H_k%*%y_k%*%t(y_k)%*%H_k)/(t(y_k)%*%H_k%*%y_k)[1] +
      (s_k%*%t(s_k))/(t(y_k)%*%s_k)[1]
    k <- k + 1
  }
  sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k , inv_hess = H_k)
  return(sol)
}

```

***
Compare those with the gradient descent, Nelder-Mead (possibly by `optim(..., method = "Nelder-Mead")`) and `R`'s own BFGS implementation (`optim(..., method = "BFGS")`) on the problems given above: What are the differences? What is the performance difference (number of iterations, execution time e.g. `system.time`)? What impact does the choice of initial matrix (and possibly using (6.20)) have on the quasi-Newton methods?

***

Undersøger antal iterationer for de forskellige metoder. Der benyttes Rosenbrock funktionen
$$f(x_1,x_2) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2 ,$$ hvor funktionens gradient er udregnet i hånden til at være
$$\nabla f(x_1,x_2) = \begin{pmatrix}2 (x_2 - 1) - 400 x_1 (x_2 -x_1^2) \\ 200 (x_2 -x_1^2) \end{pmatrix}.$$
```{r}
# Definerer funktion
f = function(x) 100*(x[2] - x[1]^2)^2 + (1 - x[1])^2
g = function(x) c(2*(x[1] - 1) - 400*x[1]*(x[2] - x[1]^2), 200*(x[2] - x[1]^2))

# Implementerer Steepest descent
steepest_descent <- function(f, g, x_k, alpha, c, rho, tol, k_max) {
  k <- 0
	g_k <- tol + 1 # For at komme i gang
	
	while ((norm(g_k,type="2") > tol) & (k < k_max)) { # Type 2 er for vektorer
		g_k <- g(x_k)
		p_k <- -g_k 
		alpha_k <- backtracking_line_search(alpha , c, rho, x_k, p_k, g_k, f)
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
	}
	sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k)
	return(sol)
}

# Vælger værdier
x_k <- c(1,2) # Valgt startværdi
alpha <- 1    
c <- 1e-4
tol <- 1e-4
rho <- 0.5
k_max <- 10000
```


Anvender metoderne på Rosenbrock funktionen som har minimum i $(1,1)$
```{r}
SD <- steepest_descent(f, g, x_k, alpha, c, rho, tol, k_max)
fDFP      <- DFP     (f, g, x_k, alpha, c, rho, tol, k_max)
fBFGS     <- BFGS    (f, g, x_k, alpha, c, rho, tol, k_max)
fBFGS_620 <- BFGS_620(f, g, x_k, alpha, c, rho, tol, k_max)
fBFGSopti <- optim   (x_k, f, g, method = "BFGS")
fNM       <- optim   (x_k, f, g, method = "Nelder-Mead")

frame <- data.frame(DFP = c(fDFP$iter,fDFP$par) , BFGS = c(fBFGS$iter,fBFGS$par), 
           BFGS_620 = c(fBFGS_620$iter,fBFGS_620$par) , 
           BFGSopt = c(NA,fBFGSopti$par) ,  NM = c(NA,fNM$par) ,
           SD = c(SD$iter,SD$par))
rownames(frame) <- c("Iter","x1","x2") ; frame 
```

Sammenligner tidsforbruget for de enkelte metoder.
```{r}
# Bruger funktionen microbenchmark til at sammenligne tidsforbeuget. 
microbenchmark(
optim(x_k, f,    method = "Nelder-Mead") , 
optim(x_k, f, g, method = "BFGS") , 
BFGS(f, g, x_k, alpha, c, rho, tol, k_max) , 
BFGS_620(f, g, x_k, alpha, c, rho, tol, k_max) ,  
DFP(f, g, x_k, alpha, c, rho, tol, k_max) ,
steepest_descent(f, g, x_k, alpha, c, rho, tol, k_max) ,  
times = 10
)
```
***
# Exercise 2: Inverse Hessian approximations

Investigate (plot, print, ...) the approximations of the Hessian ($B_{k}^{BFGS}$, $B_{k}^{DFP}$, and $B_{SR1}$) and inverse Hessian ($H_{k}^{BFGS}$, $H_{k}^{DFP}$, and $H_{SR1}$) in the optimisation problems given above. Compare to the the exact Hessian (and inverse) and maybe also to approximations by finite differences and/or automatic differentiation (possibly by libraries `numDeriv`/`madness`). (How can you summarise the difference on matrices?)

***


```{r}
# Hessematricen for Rosenbrock funktionen udregnet i hånden.
Hess = function(x) rbind(c(1200*x[1]^2 - 400*x[2] + 2, -400*x[1]), c(-400*x[1], 200))


# Definerer de inverterede hessematricer for den sidste iteration for de
# forskellige metoder, samt den inverse af den eksakte hessematrice.
HDFP      <- DFP     (f, g, x_k, alpha, c, rho, tol, k_max)$inv_hess
HBFGS     <- BFGS    (f, g, x_k, alpha, c, rho, tol, k_max)$inv_hess
HBFGS_620 <- BFGS_620(f, g, x_k, alpha, c, rho, tol, k_max)$inv_hess
H <- solve(Hess(c(1,1)))

# Definerer hessematricerne for den sidste iteration for de 
# forskellige metoder, samt den eksakte hessematrice.
BDFP      <- solve(DFP     (f, g, x_k, alpha, c, rho, tol, k_max)$inv_hess)
BBFGS     <- solve(BFGS    (f, g, x_k, alpha, c, rho, tol, k_max)$inv_hess)
BBFGS_620 <- solve(BFGS_620(f, g, x_k, alpha, c, rho, tol, k_max)$inv_hess)
B <- Hess(c(1,1))

# Afstanden indgang for indgang mellem de inverterede hessematricer fra 
# metoderne og den eksakte inverterede hessematrice.
DFP_errH     <- norm(HDFP  -     H, type = "F")
BFGS_errH    <- norm(HBFGS -     H, type = "F")
BFGS620_errH <- norm(HBFGS_620 - H, type = "F")

# Afstanden indgang for indgang mellem hessematricerne fra metoderne og 
# den eksakte hessematrice.
DFP_errB     <- norm(BDFP  -     B, type = "F")
BFGS_errB    <- norm(BBFGS -     B, type = "F")
BFGS620_errB <- norm(BBFGS_620 - B, type = "F")

frame <- data.frame(DFP = c(DFP_errH,DFP_errB) , BFGS = c(BFGS_errH,BFGS_errB) ,
           BFGS_620 = c(BFGS620_errH,BFGS620_errB))
rownames(frame) <- c("H","B") ; frame
```

***
# Exercise 3: Self-correcting properties

What happens if you introduce error in the $H_k$'s in the implementations of BFGS, DFP, and SR1? Will the method self-correct? (Discuss how to introduce error.)

Compare to methods without this error and the true inverse Hessian.

***
Der implementeres BFGS og DFP med fejl.Dette gøres, ved at den approksimerede hessematrice sættes lig en tilfældig matrice i én itteration.
```{r}
# Implementation af BFGS med fejl
BFGS_fejl <- function(f, g, x_k, alpha, c, rho, tol, k_max){
  k <- 0
  n <- length(x_k)
  H_k <- I_n <- diag(n)
  g_k <- tol + 1
  while (norm(g_k, type="2") > tol & k < k_max) {
    g_k <- g(x_k)
    p_k <- -  H_k %*% g_k
    alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
    x_k <- x_k + alpha_k * p_k
    s_k <- alpha_k * p_k
    y_k <- g(x_k) - g_k
    rho_k <- 1 / (t(y_k) %*% s_k)[1]
    if(k == 3){
      H_k <- matrix(c(-800, -8, 90, -11), nrow = 2)}
    else {
      H_k <- (I_n - rho_k*s_k%*%t(y_k))%*%H_k%*%(I_n - rho_k*y_k%*%t(s_k)) +
        rho_k*s_k%*%t(s_k)}
  
    k <- k + 1
  }
  sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k , inv_hess = H_k)
  return(sol)
}

# Implementation af DFP med fejl
DFP_fejl <- function(f, g, x_k, alpha, c, rho, tol, k_max){
  k <- 0
  n <- length(x_k)
  H_k <- I_n <- diag(n)
  g_k <- tol + 1
  while (norm(g_k, type="2") > tol & k < k_max) {
    g_k <- g(x_k)
    p_k <- -  H_k %*% g_k
    alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
   x_k <- x_k + alpha_k * p_k
    s_k <- alpha_k * p_k
    y_k <- g(x_k) - g_k
    if(k == 20){
      H_k <- matrix(c(800, -10, 0, 11), nrow = 2)}
    else {
    H_k <- H_k - (H_k%*%y_k%*%t(y_k)%*%H_k)/(t(y_k)%*%H_k%*%y_k)[1] +
      (s_k%*%t(s_k))/(t(y_k)%*%s_k)[1]}
    k <- k + 1
  }
  sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k , inv_hess = H_k)
  return(sol)
}

# Implementation af Newton
Newton <- function(f, g, x_k, alpha, c, rho, tol, k_max) {
  k <- 0
	g_k <- tol + 1 # For at komme i gang
	while ((norm(g_k,type="2") > tol) & (k < k_max)) { # Type 2 er for vektorer
		g_k <- g(x_k)
		p_k <- -solve(Hess(x_k)) %*% g_k # Bruger den eksakte Hessian
		alpha_k <- backtracking_line_search(alpha , c, rho, x_k, p_k, g_k, f)
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
	} 
	sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k, 
	            inv_hess = solve(Hess(x_k)))
	return(sol)
}

```

Metoderne med fejl sammenlignes med metoderne uden fejl, samt med Newton-metoden hvor den eksakte hessematrice indgår.

```{r}
# Vælger værdier
x_k <- c(0,0)
alpha <- 1    
c <- 1e-4
tol <- 1e-4
rho <- 0.5
k_max <- 10000

# Sammenligner metoderne med fejl med metoderne uden fejl, samt med Newton.
BFGS(f, g, x_k, alpha, c, rho, tol, k_max)
BFGS_fejl    (f, g, x_k, alpha, c, rho, tol, k_max)
DFP(f, g, x_k, alpha, c, rho, tol, k_max)
DFP_fejl(f, g, x_k, alpha, c, rho, tol, k_max)
Newton(f, g, x_k, alpha, c, rho, tol, k_max)


# Bruger funktionen microbenchmark til at sammenligne tidsforbeuget. 
microbenchmark(
BFGS         (f, g, x_k, alpha, c, rho, tol, k_max),
BFGS_fejl    (f, g, x_k, alpha, c, rho, tol, k_max),
DFP          (f, g, x_k, alpha, c, rho, tol, k_max),
DFP_fejl     (f, g, x_k, alpha, c, rho, tol, k_max),
Newton       (f, g, x_k, alpha, c, rho, tol, k_max),
times = 10
)
```