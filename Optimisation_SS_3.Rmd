---
title: "Optimisation: Self study 3 -- Numerical differentiation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
printf <- function(...) cat(sprintf(...))
printfln <- function(...) cat(sprintf(...), "\n")
library(microbenchmark)
```

# Exercise 1: BFGS, DFP and SR1

***
Implement BFGS, DFP and SR1 (as line search).

***
_Line Search iteration_\
\[
x_{k+1}=x_k+\alpha_kp_k,
\]
hvor $\alpha_k$ er skridtlængden som skal opfylde _Wolfe conditions_, og søgningsretningen $p_k$ er givet ved\
\[
    p_k=-B_k^{-1}\nabla f(x_k).
\]

_Sekantligning_\
\[
    B_{k+1}s_k=y_k,
\]
hvor 
\[
    s_k = x_{k+1} - x_k = \alpha_k p_k, 
    y_k = \nabla f_{k+1} - \nabla f_k.
\]

_Curvature betingelsen_\
\[
    s_k^\top y_k > 0.
\]

Betragter forskellige _Quasi-Newton metoder_.

**DFP** opdateringsformel\
\[
  B_{k+1} = (I -\rho_k y_k s_k^\top) B_k (I -\rho_k s_k y_k^\top) + \rho_k y_k y_k^\top.\tag{DFP}
\]
Inverteret formel\
\[
     H_{k+1} = H_k - \frac{H_k y_k y_k^\top H_k}{y_k^\top H_k y_k} + \frac{s_k s_k^\top}{y_k^\top}. \tag{DFP}
\]

**BFGS Metoden** opdaterings formel\
\[
    H_{k+1} = (I -\rho_k y_k s_k^\top) H_k (I -\rho_k s_k y_k^\top) + \rho_k y_k y_k^\top.\tag{BFGS}
\]
 
**SR1** opdateringsformel\
\[
    H_{k+1} = H_k + \frac{(s_k - H_ky_k) (s_k-H_ky_k)^\top}{(s_k - H_ky_k)^\top y_k}\tag{SR1}
\]


```{r}
# Implementation af backtracking
backtracking_line_search <- function(alpha, c, rho, x_k, p_k, g_k, f) {
  alpha_k <- alpha
	repeat { 
		lhs <- f(x_k + alpha_k * p_k)
		rhs <- f(x_k) + c * alpha_k * g_k %*% p_k
		if (lhs <= rhs) break
		alpha_k <- rho * alpha_k
	}
	return(alpha_k)
}

# Implementation af BFGS
BFGS <- function(f, g, x_k, alpha, c, rho, tol, k_max){
  k <- 0
  n <- length(x_k)
  H_k <- I_n <- diag(n)
  g_k <- tol + 1
  while (norm(g_k, type="2") > tol & k < k_max) {
    g_k <- g(x_k)
    p_k <- -  H_k %*% g_k
    alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
    x_k <- x_k + alpha_k * p_k
    s_k <- alpha_k * p_k
    y_k <- g(x_k) - g_k
    rho_k <- 1 / (t(y_k) %*% s_k)[1]
    H_k <- (I_n - rho_k*s_k%*%t(y_k))%*%H_k%*%(I_n - rho_k*y_k%*%t(s_k)) + rho_k*s_k%*%t(s_k)
    k <- k + 1
  }
  sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k , hess = H_k)
  return(sol)
}

# Implementation af BFGS opdatering
BFGS_620 <- function(f, g, x_k, alpha, c, rho, tol, k_max){
  n <- length(x_k)
  H_k <- I_n <- diag(n)
  g_k <- g(x_k)
    p_k <- -  H_k %*% g_k
    alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
    x_k <- x_k + alpha_k * p_k
    s_k <- alpha_k * p_k
    y_k <- g(x_k) - g_k
    H_k <- ((t(y_k) %*% s_k)[1]) / ((t(y_k) %*% y_k)[1]) * I_n
    k <- 1
  while (norm(g_k, type="2") > tol & k < k_max) {
    g_k <- g(x_k)
    p_k <- -  H_k %*% g_k
    alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
    x_k <- x_k + alpha_k * p_k
    s_k <- alpha_k * p_k
    y_k <- g(x_k) - g_k
    rho_k <- 1 / (t(y_k) %*% s_k)[1]
    H_k <- (I_n - rho_k*s_k%*%t(y_k))%*%H_k%*%(I_n - rho_k*y_k%*%t(s_k)) + rho_k*s_k%*%t(s_k)
    k <- k + 1
  }
sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k , hess = H_k)
  return(sol)
}

# Implementation af DFP
DFP <- function(f, g, x_k, alpha, c, rho, tol, k_max){
  k <- 0
  n <- length(x_k)
  H_k <- I_n <- diag(n)
  g_k <- tol + 1
  while (norm(g_k, type="2") > tol & k < k_max) {
    g_k <- g(x_k)
    p_k <- -  H_k %*% g_k
    alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
   x_k <- x_k + alpha_k * p_k
    s_k <- alpha_k * p_k
    y_k <- g(x_k) - g_k
    H_k <- H_k - (H_k%*%y_k%*%t(y_k)%*%H_k)/(t(y_k)%*%H_k%*%y_k)[1] + (s_k%*%t(s_k))/(t(y_k)%*%s_k)[1]
    k <- k + 1
  }
  sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k , hess = H_k)
  return(sol)
}

```


Undersøger antal iteration for de forskellige metoder. Der benyttes funktionen
$$f(x_1,x_2) = \frac{1}{2}(100x_1^2+x_2^2),$$ hvor funktionens gradient er udregnet i hånden til at være
$$\nabla f(x_1,x_2) = \begin{pmatrix}100x_1 \\ x_2\end{pmatrix}.$$
```{r}
# f = function(x) 0.5*(100*x[1]^2 + x[2]^2)
# g = function(x) c(100*x[1], x[2])

f = function(x) 100*(x[2] - x[1]^2)^2 + (1 - x[1])^2
g = function(x) c(2*(x[1] - 1) - 400*x[1]*(x[2] - x[1]^2), 200*(x[2] - x[1]^2))

x_k <- c(1,2) # Valgt startværdi
alpha <- 1    
c <- 1e-4
tol <- 1e-4
rho <- 0.5
k_max <- 10000


DFP     (f, g, x_k, alpha, c, rho, tol, k_max)$par
BFGS    (f, g, x_k, alpha, c, rho, tol, k_max)$par
BFGS_620(f, g, x_k, alpha, c, rho, tol, k_max)$par
optim   (x_k, f, g, method = "BFGS")$par

DFP     (f, g, x_k, alpha, c, rho, tol, k_max)$iter
BFGS    (f, g, x_k, alpha, c, rho, tol, k_max)$iter
BFGS_620(f, g, x_k, alpha, c, rho, tol, k_max)$iter
optim   (x_k, f, g, method = "BFGS")$counts

```
***
Compare those with the gradient descent, Nelder-Mead (possibly by `optim(..., method = "Nelder-Mead")`) and `R`'s own BFGS implementation (`optim(..., method = "BFGS")`) on the problems given above: What are the differences? What is the performance difference (number of iterations, execution time e.g. `system.time`)? What impact does the choice of initial matrix (and possibly using (6.20)) have on the quasi-Newton methods?

Remember the DRY principle (don't repeat yourself). Here is a example of how to plot several lines in a figure:

***

```{r}
steepest_descent <- function(f, g, x_k, alpha, c, rho, tol, k_max) {
  k <- 0
	g_k <- tol + 1 # For at komme i gang
	
	while ((norm(g_k,type="2") > tol) & (k < k_max)) { # Type 2 er for vektorer
		g_k <- g(x_k)
		p_k <- -g_k 
		alpha_k <- backtracking_line_search(alpha , c, rho, x_k, p_k, g_k, f)
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
	}
	sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k)
	return(sol)
}

microbenchmark(
optim(x_k, f,    method = "Nelder-Mead") , 
optim(x_k, f, g, method = "BFGS") , 
BFGS(f, g, x_k, alpha, c, rho, tol, k_max) , 
BFGS_620(f, g, x_k, alpha, c, rho, tol, k_max) ,  
DFP(f, g, x_k, alpha, c, rho, tol, k_max) ,
steepest_descent(f, g, x_k, alpha, c, rho, tol, k_max) ,  
times = 10
)
```
***
# Exercise 2: Inverse Hessian approximations

Investigate (plot, print, ...) the approximations of the Hessian ($B_{k}^{BFGS}$, $B_{k}^{DFP}$, and $B_{SR1}$) and inverse Hessian ($H_{k}^{BFGS}$, $H_{k}^{DFP}$, and $H_{SR1}$) in the optimisation problems given above. Compare to the the exact Hessian (and inverse) and maybe also to approximations by finite differences and/or automatic differentiation (possibly by libraries `numDeriv`/`madness`). (How can you summarise the difference on matrices?)

***
```{r}
Hess = function(x) rbind(c(1200*x[1]^2 - 400*x[2] + 2, -400*x[1]), c(-400*x[1], 200))

HDFP      <- DFP     (f, g, x_k, alpha, c, rho, tol, k_max)$hess
HBFGS     <- BFGS    (f, g, x_k, alpha, c, rho, tol, k_max)$hess
HBFGS_620 <- BFGS_620(f, g, x_k, alpha, c, rho, tol, k_max)$hess
H <- solve(Hess(c(1,1)))


BDFP      <- solve(DFP     (f, g, x_k, alpha, c, rho, tol, k_max)$hess)
BBFGS     <- solve(BFGS    (f, g, x_k, alpha, c, rho, tol, k_max)$hess)
BBFGS_620 <- solve(BFGS_620(f, g, x_k, alpha, c, rho, tol, k_max)$hess)
B <- Hess(c(1,1))

H
abs(HDFP  -     H)
abs(HBFGS -     H)
abs(HBFGS_620 - H)

B
abs(BDFP  -     B)
abs(BBFGS -     B)
abs(BBFGS_620 - B)
```


# Exercise 3: Self-correcting properties

What happens if you introduce error in the $H_k$'s in the implementations of BFGS, DFP, and SR1? Will the method self-correct? (Discuss how to introduce error.)

Compare to methods without this error and the true inverse Hessian.


# Exercise 4: Be creative!

If you have anything, put it here.