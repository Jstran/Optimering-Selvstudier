---
title: "Optimisation: Self study 1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("animation")
```


```{r}
plot(dist ~ speed, cars)
```

Denote by $(s_i, d_i)$ for $i = 1, 2, \ldots, `r nrow(cars)`$ the observations.

We want to fit a straight line of the form $m(s) = a + b \cdot s$ to the data. We want to determine $a$ and $b$. One way is to minimise the objective function given by
\[
f(a, b) = \frac{1}{n} \sum_{i = 1}^n f_i(a, b),
\]
where
\[
f_i(a, b) = (m(s_i) - d_i)^2 .
\]

## Animation

Below, you are asked to illustrate. You can try both static graphics and with animations (e.g. using the `animation` package: <https://yihui.name/animation/>).

# Exercise 1: Gradient descent

Exercises:


***
1. What is the gradient of $f$?

***
```{r}
f <- function(ab){
  sum = 0
  n <- length(cars$dist)
  s <- cars$speed
  d <- cars$dist
  for (i in 1:n){ 
    sum <- sum + ((ab[1] + ab[2] * s[i]) - d[i])^2}
  return(sum * 1/n)
}
```

Gradienten er udregnet i hånden til at være
\[
\nabla f(a, b) = \begin{pmatrix}1/n \sum^n_{i=1}2((a+bs_i)-d_i)\\ 1/n\sum^n_{i=1}2((a+bs_i)-d_i)s_i\end{pmatrix},
\]

Funktionen defineres i R
```{r}
g <- function(ab){
  n <- length(cars$speed)
  s <- cars$speed
  d <- cars$dist
  g_a <- g_b <- numeric(n) 
  grad <- numeric(2)
  for (i in 1:n){
    g_a[i] <- 2*(ab[1] + ab[2] * s[i] - d[i])
    g_b[i] <- 2*(ab[1] + ab[2] * s[i] - d[i])*s[i]
  }
  grad[1] <- 1/n*sum(g_a)
  grad[2] <- 1/n*sum(g_b)
  return(grad)
}
```

***
2. Implement gradient descent and use it to find the best straight line.

***

For at implementere gradient descent, implementeres først backtracking. Funktionen defineres som følgende
```{r}
backtracking_line_search <- function(alpha, c, rho, x_k, p_k, g_k, f) {
	alpha_k <- alpha
	repeat { 
		lhs <- f(x_k + alpha_k * p_k)
		rhs <- f(x_k) + c * alpha_k * g_k %*% p_k
		if (lhs <= rhs) break
		alpha_k <- rho * alpha_k
	}
	return(alpha_k)
}
```


```{r}
printf <- function(...) cat(sprintf(...))
printfln <- function(...) cat(sprintf(...), "\n")

steepest_descent <- function(f, g, x_k, alpha, c, rho, tol, k_max) {
	k <- 0
	g_k <- tol + 1 # to get started
	
	while ((abs(g_k) > tol) && (k < k_max)) {
		g_k <- g(x_k)
		p_k <- -g_k 
		alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
	}
	
	printfln("k = %d \t x_k = (%6.3f , %6.3f) \t f_k = %.2f \t g_k = (%8.3f , %8.3f) \t alpha_k = %.5f", k, x_k[1] , x_k[2], f(x_k), g_k[1] , g_k[2], alpha_k)
	
	return(x_k)
}

x_sol <- steepest_descent(f, g, x_k   = c(3,3) , 
                                alpha = 0.1    ,
                                c     = 1e-4   , 
                                rho   = 0.75   , 
                                tol   = 1e-4   , 
                                k_max = 100)

plot(dist ~ speed , cars)
abline(x_sol , col = "red")

#fplot <- function(a,b){return(f(c(a,b)))}
#gplot <- function(a,b){return(g(c(a,b)))}

#h <- function(x,y) 2*x^2 + 5*y^2
#ani.options(interval = 0.3, nmax = 50)
#xx = grad.desc(fplot , gplot)
#xx$par 

```



    * What is meant by *the best* straight line in relation to the objective function above?
    * Discuss different ways to determine the step sizes.
3. Try with different ways to choose step sizes and illustrate it (including plotting the objective function and the iterates, $\{x_k\}_k$). 
    * (Technically, it may be easier to have an algorithm for each way of choosing step size.)
4. Show some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`).

Account for theoretical properties of the gradient descent.

# Exercise 2: Stochastic gradient descent / incremental gradient descent

In the gradient descent method, all observations are used in each step. If the dataset is really big it may be a problem.

Instead, many smaller steps can be taken (either using one observation at a time or small batches of observations). This is often called stochastic gradient descent or incremental gradient descent and can be described as:

* Choose starting value $x_0$ ($x_0 = (a_0, b_0)$).
* Repeat until convergence:
    + Randomly shuffle the observations in the dataset with a permutation $\sigma$ such that observation $i$ now becomes observation $\sigma(i)$.
    + For each $i = 1, 2, \ldots, n$: take a step using only the $\sigma(i)$'th observation (minimise $f_{\sigma(i)}$ instead of $f$).

Exercises:

1. What is the difference between stochastic gradient descent and gradient descent?
2. (Optional) Implement stochastic gradient descent. 
3. (Optional) Illustrate the behaviour of the stochastic gradient descent, including:
    + Different ways to choose step sizes.
    + The total objective function with a discussion of how it differs from a similar plot from the gradient descent method.
    + Some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`).

# Exercise 3: Be creative!

Open exercise: try to be creative!