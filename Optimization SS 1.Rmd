---
title: "Optimisation: Self study 1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("animation")
Outer <- function(x,y,fun) {
   mat <- matrix(NA, length(x), length(y))
   for (i in seq_along(x)) {
            for (j in seq_along(y)) {mat[i,j] <- fun(x[i],y[j])} }
   mat}
printf <- function(...) cat(sprintf(...))
printfln <- function(...) cat(sprintf(...), "\n")
```


```{r}
plot(dist ~ speed, cars)
```

Denote by $(s_i, d_i)$ for $i = 1, 2, \ldots, `r nrow(cars)`$ the observations.

We want to fit a straight line of the form $m(s) = a + b \cdot s$ to the data. We want to determine $a$ and $b$. One way is to minimise the objective function given by
\[
f(a, b) = \frac{1}{n} \sum_{i = 1}^n f_i(a, b),
\]
where
\[
f_i(a, b) = (m(s_i) - d_i)^2 .
\]

## Animation

Below, you are asked to illustrate. You can try both static graphics and with animations (e.g. using the `animation` package: <https://yihui.name/animation/>).

# Exercise 1: Gradient descent

Exercises:


***
1. What is the gradient of $f$?

***
```{r}
f <- function(ab){
  n <- length(cars$dist)
  s <- cars$speed
  d <- cars$dist
  return(1/n * sum((ab[1] + ab[2] * s - d)^2))
}

f_xy <- function(x,y) return(f(c(x,y)))
```

Gradienten er udregnet i hånden til at være
\[
\nabla f(a, b) = \begin{pmatrix}1/n \sum^n_{i=1}2((a+bs_i)-d_i)\\ 1/n\sum^n_{i=1}2((a+bs_i)-d_i)s_i\end{pmatrix},
\]

Funktionen defineres i R
```{r}
g <- function(ab){
  n <- length(cars$speed)
  s <- cars$speed
  d <- cars$dist 
  g_a <- 1/n * sum(2 * (ab[1] + ab[2] * s - d) )
  g_b <- 1/n * sum(2 * (ab[1] + ab[2] * s - d) * s)
  return(c(g_a,g_b))
}
```

***
2. Implement gradient descent and use it to find the best straight line.

***

For at implementere gradient descent, implementeres først backtracking. Funktionen defineres som følgende
```{r}
backtracking_line_search <- function(alpha, c, rho, x_k, p_k, g_k, f) {
	alpha_k <- alpha
	repeat { 
		lhs <- f(x_k + alpha_k * p_k)
		rhs <- f(x_k) + c * alpha_k * g_k %*% p_k
		if (lhs <= rhs) break
		alpha_k <- rho * alpha_k
	}
	return(alpha_k)
}
```


```{r}
steepest_descent <- function(f, g, x_k, alpha, c, rho, tol, k_max) {
	
  N <- 30
	x <- seq(-20,3,length=N)
	y <- seq(1,7,  length=N)
  z <- Outer(x , y , f_xy)
  lev <- c(300,500,750,1000,2000,4000)
  contour(x, y, z, levels = lev)
  
  k <- 0
	g_k <- tol + 1 # For at komme i gang
	while ((norm(g_k , type="2") > tol) & (k < k_max)) { # Type 2 er for vektorer
		g_k <- g(x_k)
		p_k <- -g_k 
		alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
		x_old <- x_k
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
		
		lines(c(x_old[1],x_k[1]) , c(x_old[2],x_k[2]) , col = "red")
	}
	
	printfln("k = %d \t x_k = (%6.3f , %6.3f) \t f_k = %.2f \t g_k = (%8.3f , %8.3f) \t alpha_k = %.5f", k, x_k[1] , x_k[2], f(x_k), g_k[1] , g_k[2], alpha_k)
	
	return(x_k)
}

x_sol <- steepest_descent(f, g, x_k   = c(1,5) , 
                                alpha = 1      ,
                                c     = 1e-4   , 
                                rho   = 0.5    , 
                                tol   = 1e-4   , 
                                k_max = 20000)
```

Bruges de funde værdier for a og b som koefficienter for den rette linje $m(s) = a + b \cdot s$ opnås følgende
```{r}
plot(dist ~ speed , cars)
abline(x_sol , col = "red")
legend(4,115,legend = "m(s) = a + bs" , col = "red" , lty = 1)
```


***
What is meant by *the best* straight line in relation to the objective function above?

***

Objektfunktionen er givet ved summen af de kvadrerede residualer ganget med 1/n, og den bedste rette linje opnås netop ved de koefficienter der minimerer denne sum.  

***
Discuss different ways to determine the step sizes.

***
    
I stedet for at bruge Backtracking til at bestemme skridtlængden kunne der i stedet benyttes zoom. Denne metode ville formendelig gøre, at Steepest Descent konvergerede hurtigere, idet zoom vil gøre at skridtlængden overholder strong Wolf, og dermed også sikre at skridtlængderne ikke er for små, hvilket Backtracking ikke nødvendigvis gør.

Udover zoom kan der også benyttes en fast skridtlængde. Dette vil dog gøre at Steepest Descent ikke nødvendigvis vil konvergere.


***
3. Try with different ways to choose step sizes and illustrate it (including plotting the objective function and the iterates, $\{x_k\}_k$). 
    * (Technically, it may be easier to have an algorithm for each way of choosing step size.)

***

***
4. Show some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`).
Account for theoretical properties of the gradient descent.

***

Dette ses af konturplottet hvor iterationerne for Steepest Descent er tegnet ind. Det er ikke så nemt at se på ovenstående plot, men i Steepest Descent vil søgningsretningen, $p_k$, stå ortogonalt på niveaukurverne i konturplottet.

# Exercise 2: Stochastic gradient descent / incremental gradient descent

In the gradient descent method, all observations are used in each step. If the dataset is really big it may be a problem.

Instead, many smaller steps can be taken (either using one observation at a time or small batches of observations). This is often called stochastic gradient descent or incremental gradient descent and can be described as:

* Choose starting value $x_0$ ($x_0 = (a_0, b_0)$).
* Repeat until convergence:
    + Randomly shuffle the observations in the dataset with a permutation $\sigma$ such that observation $i$ now becomes observation $\sigma(i)$.
    + For each $i = 1, 2, \ldots, n$: take a step using only the $\sigma(i)$'th observation (minimise $f_{\sigma(i)}$ instead of $f$).

Exercises:

1. What is the difference between stochastic gradient descent and gradient descent?
2. (Optional) Implement stochastic gradient descent. 
3. (Optional) Illustrate the behaviour of the stochastic gradient descent, including:
    + Different ways to choose step sizes.
    + The total objective function with a discussion of how it differs from a similar plot from the gradient descent method.
    + Some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`).