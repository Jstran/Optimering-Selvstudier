---
title: 'Optimisation: Self study 1'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("animation")
Outer <- function(x,y,fun) {
   mat <- matrix(NA, length(x), length(y))
   for (i in seq_along(x)) {
            for (j in seq_along(y)) {mat[i,j] <- fun(x[i],y[j])} }
   mat}
printf <- function(...) cat(sprintf(...))
printfln <- function(...) cat(sprintf(...), "\n")
```


```{r}
plot(dist ~ speed, cars)
```

Denote by $(s_i, d_i)$ for $i = 1, 2, \ldots, `r nrow(cars)`$ the observations.

We want to fit a straight line of the form $m(s) = a + b \cdot s$ to the data. We want to determine $a$ and $b$. One way is to minimise the objective function given by
\[
f(a, b) = \frac{1}{n} \sum_{i = 1}^n f_i(a, b),
\]
where
\[
f_i(a, b) = (m(s_i) - d_i)^2 .
\]

## Animation

Below, you are asked to illustrate. You can try both static graphics and with animations (e.g. using the `animation` package: <https://yihui.name/animation/>).

# Exercise 1: Gradient descent

Exercises:


***
1. What is the gradient of $f$?

***

Definerer først objektfunktionen i `R`.
```{r}
f <- function(ab){
  n <- length(cars$dist)
  s <- cars$speed
  d <- cars$dist
  return(1/n * sum((ab[1] + ab[2] * s - d)^2))
}

f_xy <- function(x,y) return(f(c(x,y)))
```

Gradienten er udregnet i hånden til at være
\[
\nabla f(a, b) = \begin{pmatrix}1/n \sum^n_{i=1}2((a+bs_i)-d_i)\\ 1/n\sum^n_{i=1}2((a+bs_i)-d_i)s_i\end{pmatrix},
\]

Gradienten kan nu defineres i `R`
```{r}
g <- function(ab){
  n <- length(cars$speed)
  s <- cars$speed
  d <- cars$dist 
  g_a <- 1/n * sum(2 * (ab[1] + ab[2] * s - d) )
  g_b <- 1/n * sum(2 * (ab[1] + ab[2] * s - d) * s)
  return(c(g_a,g_b))
}
```

***
2. Implement gradient descent and use it to find the best straight line.

***
I Line search er iterationerne givet ved 
$$x_{k+1} = x_k + \alpha_k p_k,$$
hvor $\alpha_k$ er skridtlængden og $p_k$ er søgningsretningen.

Ud fra Taylors sætning haves det at objektfunktionen kan skrives som
\begin{align*}
f(x_k+\alpha_kp_k)\approx f(x_k)+\alpha_kp_k^\top \nabla f_k,
\end{align*}
hvor det kan ses at hvis $p_k^\top\nabla f_k<0$, reduceres $f$ i denne retning. Det kan vises at $f$ reduceres mest for 
$$p_k=-\nabla f_k.$$
Denne retning kaldes *steepest descent*.

Den skridtlængde der er mest optimal at vælge, er den der minimerer 
$$f(x_k+\alpha p_k).$$
Dette er kan dog være dyrt at bestemme og derfor kan der istedet pålægges $\alpha_k$ nogle betingelser.

**Wolfe Conditions**
\begin{align}
    f(x_k+\alpha_kp_k) &\leq f(x_k)+c_1\alpha \nabla f_k^\top p_k,\\
    \nabla f(x_k+\alpha_k p_k)^\top p_k &\geq c_2\nabla f_k^\top p_k,
\end{align}
hvor $0 < c_1 < c_2 < 1$.

En metoder der bestemmer $\alpha_k$ baseret på den første Wolfe betingelse er Backtracking.
Backtracking implementeres som følgende.
```{r}
# Implemation af backtracking.
backtracking_line_search <- function(alpha, c, rho, x_k, p_k, g_k, f) {
	alpha_k <- alpha
	repeat { 
		lhs <- f(x_k + alpha_k * p_k)
		rhs <- f(x_k) + c * alpha_k * g_k %*% p_k
		if (lhs <= rhs) break
		alpha_k <- rho * alpha_k
	}
	return(alpha_k)
}
```

Nu hvor der er fundet en metode til at vælge skridtlængden $\alpha_k$, kan Steepest Descent implementeres. Her benyttes søgningsretningen $p_k = - \nabla f_k$ i hver iteration.

```{r}
# Implemantation af Steepest Descent.
steepest_descent <- function(f, g, x_k, alpha, c, rho, tol, k_max) {
	
  N <- 30 
	x <- seq(-20,3,length=N)
	y <- seq(1,7,  length=N)
  z <- Outer(x , y , f_xy)
  lev <- c(300,500,750,1000,2000,4000)
  contour(x, y, z, levels = lev)
  
  k <- 0
	g_k <- tol + 1 # For at komme i gang
	while ((norm(g_k , type="2") > tol) & (k < k_max)) { # Type 2 er for vektorer
		g_k <- g(x_k)
		p_k <- -g_k 
		alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
		x_old <- x_k
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
		
		lines(c(x_old[1],x_k[1]) , c(x_old[2],x_k[2]) , col = "red")
	}
	
	printfln("k = %d \t x_k = (%6.3f , %6.3f) \t f_k = %.2f \t g_k = (%8.3f , %8.3f) \t alpha_k = %.5f", k, x_k[1] , x_k[2], f(x_k), g_k[1] , g_k[2], alpha_k)
	
	return(x_k)
}
```

Steepest Descent andvendes på objektfunktionen.

```{r}
# Vælger værdier.
x_k   <- c(1,5)
alpha <-  1              
c     <-  1e-4 
rho   <-  0.5
tol   <-  1e-4
k_max <-  20000

x_sol <- steepest_descent(f, g, x_k, alpha, c, rho, tol, k_max); x_sol
```

Bruges de funde værdier for $a$ og $b$ som koefficienter for den rette linje $m(s) = a + b \cdot s$ opnås følgende.
```{r}
plot(dist ~ speed , cars)
abline(x_sol , col = "red")
legend(4 , 115 , legend = "m(s) = a + bs" , col = "red" , lty = 1)
```


***
What is meant by *the best* straight line in relation to the objective function above?

***

Objektfunktionen er givet ved summen af de kvadrerede residualer ganget med $1/n$, og den bedste rette linje opnås netop ved de koefficienter der minimerer denne sum.  

***
Discuss different ways to determine the step sizes.

***
    
I stedet for at bruge Backtracking til at bestemme skridtlængden kunne der i stedet benyttes zoom. Denne metode ville formendelig gøre, at Steepest Descent konvergerede hurtigere, idet zoom vil gøre at skridtlængden overholder strong Wolfe, og dermed også sikre at skridtlængderne ikke er for små, hvilket Backtracking ikke nødvendigvis gør.

Udover zoom kan der også benyttes en fast skridtlængde. Dette vil dog gøre at Steepest Descent ikke nødvendigvis vil konvergere.


***
3. Try with different ways to choose step sizes and illustrate it (including plotting the objective function and the iterates, $\{x_k\}_k$). 
    * (Technically, it may be easier to have an algorithm for each way of choosing step size.)

***

***
4. Show some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`).
Account for theoretical properties of the gradient descent.

***

Dette ses af konturplottet hvor iterationerne for Steepest Descent er tegnet ind. Det er ikke så nemt at se på ovenstående plot, men i Steepest Descent vil søgningsretningen, $p_k$, stå ortogonalt på niveaukurverne i konturplottet.

***

# Exercise 2: Stochastic gradient descent / incremental gradient descent

In the gradient descent method, all observations are used in each step. If the dataset is really big it may be a problem.

Instead, many smaller steps can be taken (either using one observation at a time or small batches of observations). This is often called stochastic gradient descent or incremental gradient descent and can be described as:

* Choose starting value $x_0$ ($x_0 = (a_0, b_0)$).
* Repeat until convergence:
    + Randomly shuffle the observations in the dataset with a permutation $\sigma$ such that observation $i$ now becomes observation $\sigma(i)$.
    + For each $i = 1, 2, \ldots, n$: take a step using only the $\sigma(i)$'th observation (minimise $f_{\sigma(i)}$ instead of $f$).

Exercises:

1. What is the difference between stochastic gradient descent and gradient descent?

***

I Gradient Descent benyttes alle punkter for hver iteration, hvilket kan være et problem når der arbejdes med store datasæt, idet det kan kræve meget computerkapacitet. I Stokastisk Gradient Descent tager man i stedet skridt baseret på én eller få observationer. Denne metode vil derfor kræve mindre computerkapacitet.  

