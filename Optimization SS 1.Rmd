---
title: "Optimisation: Self study 1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("animation")
Outer <- function(x,y,fun) {
   mat <- matrix(NA, length(x), length(y))
   for (i in seq_along(x)) {
            for (j in seq_along(y)) {mat[i,j] <- fun(x[i],y[j])} }
   mat}
printf <- function(...) cat(sprintf(...))
printfln <- function(...) cat(sprintf(...), "\n")
```


```{r}
plot(dist ~ speed, cars)
```

Denote by $(s_i, d_i)$ for $i = 1, 2, \ldots, `r nrow(cars)`$ the observations.

We want to fit a straight line of the form $m(s) = a + b \cdot s$ to the data. We want to determine $a$ and $b$. One way is to minimise the objective function given by
\[
f(a, b) = \frac{1}{n} \sum_{i = 1}^n f_i(a, b),
\]
where
\[
f_i(a, b) = (m(s_i) - d_i)^2 .
\]

## Animation

Below, you are asked to illustrate. You can try both static graphics and with animations (e.g. using the `animation` package: <https://yihui.name/animation/>).

# Exercise 1: Gradient descent

Exercises:


***
1. What is the gradient of $f$?

***
```{r}
f <- function(ab){
  sum = 0
  n <- length(cars$dist)
  s <- cars$speed
  d <- cars$dist
  return(1/n * sum((ab[1] + ab[2] * s - d)^2))
}

f_xy <- function(x,y) return(f(c(x,y)))
```

Gradienten er udregnet i hånden til at være
\[
\nabla f(a, b) = \begin{pmatrix}1/n \sum^n_{i=1}2((a+bs_i)-d_i)\\ 1/n\sum^n_{i=1}2((a+bs_i)-d_i)s_i\end{pmatrix},
\]

Funktionen defineres i R
```{r}
g <- function(ab){
  n <- length(cars$speed)
  s <- cars$speed
  d <- cars$dist 
  g_a <- 1/n * sum(2 * (ab[1] + ab[2] * s - d) )
  g_b <- 1/n * sum(2 * (ab[1] + ab[2] * s - d) * s)
  return(c(g_a,g_b))
}
```

***
2. Implement gradient descent and use it to find the best straight line.

***

For at implementere gradient descent, implementeres først backtracking. Funktionen defineres som følgende
```{r}
backtracking_line_search <- function(alpha, c, rho, x_k, p_k, g_k, f) {
	alpha_k <- alpha
	repeat { 
		lhs <- f(x_k + alpha_k * p_k)
		rhs <- f(x_k) + c * alpha_k * g_k %*% p_k
		if (lhs <= rhs) break
		alpha_k <- rho * alpha_k
	}
	return(alpha_k)
}
```


```{r}
steepest_descent <- function(f, g, x_k, alpha, c, rho, tol, k_max) {
	
  N <- 30
	x <- seq(-20,3,length=N)
	y <- seq(1,7,  length=N)
  z <- Outer(x , y , f_xy)
  lev <- c(300,500,750,1000,2000,4000)
  contour(x, y, z, levels = lev)
  
  k <- 0
	g_k <- tol + 1 # For at komme i gang
	while ((norm(g_k , type="2") > tol) & (k < k_max)) { # Type 2 er for vektorer
		g_k <- g(x_k)
		p_k <- -g_k 
		alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
		x_old <- x_k
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
		
		lines(c(x_old[1],x_k[1]) , c(x_old[2],x_k[2]) , col = "red")
	}
	
	printfln("k = %d \t x_k = (%6.3f , %6.3f) \t f_k = %.2f \t g_k = (%8.3f , %8.3f) \t alpha_k = %.5f", k, x_k[1] , x_k[2], f(x_k), g_k[1] , g_k[2], alpha_k)
	
	return(x_k)
}

x_sol <- steepest_descent(f, g, x_k   = c(1,5) , 
                                alpha = 1      ,
                                c     = 1e-4   , 
                                rho   = 0.5    , 
                                tol   = 1e-4   , 
                                k_max = 20000)
```

Bruges de funde værdier for a og b som koefficienter for den rette linje $m(s) = a + b \cdot s$ opnås følgende
```{r}
plot(dist ~ speed , cars)
abline(x_sol , col = "red")
legend(4,115,legend = "m(s) = a + bs" , col = "red" , lty = 1)
```



    * What is meant by *the best* straight line in relation to the objective function above?
Objektfunktionen er givet ved summen af de kvadrerede residualer ganget med 1/n, og den bedste rette linje opnås netop ved de koefficienter der minimerer denne sum.  

    * Discuss different ways to determine the step sizes.
***
3. Try with different ways to choose step sizes and illustrate it (including plotting the objective function and the iterates, $\{x_k\}_k$). 
    * (Technically, it may be easier to have an algorithm for each way of choosing step size.)

***

***
4. Show some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`).
Account for theoretical properties of the gradient descent.

***
# Exercise 2: Stochastic gradient descent / incremental gradient descent

In the gradient descent method, all observations are used in each step. If the dataset is really big it may be a problem.

Instead, many smaller steps can be taken (either using one observation at a time or small batches of observations). This is often called stochastic gradient descent or incremental gradient descent and can be described as:

* Choose starting value $x_0$ ($x_0 = (a_0, b_0)$).
* Repeat until convergence:
    + Randomly shuffle the observations in the dataset with a permutation $\sigma$ such that observation $i$ now becomes observation $\sigma(i)$.
    + For each $i = 1, 2, \ldots, n$: take a step using only the $\sigma(i)$'th observation (minimise $f_{\sigma(i)}$ instead of $f$).

Exercises:

1. What is the difference between stochastic gradient descent and gradient descent?
2. (Optional) Implement stochastic gradient descent. 
3. (Optional) Illustrate the behaviour of the stochastic gradient descent, including:
    + Different ways to choose step sizes.
    + The total objective function with a discussion of how it differs from a similar plot from the gradient descent method.
    + Some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`).

# Exercise 3: Be creative!

Open exercise: try to be creative!