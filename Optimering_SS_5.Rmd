---
title: 'Optimisation: Self study 5 - Conjugate gradient'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
printf <- function(...) cat(sprintf(...))
printfln <- function(...) cat(sprintf(...), "\n")
library(microbenchmark)
```
***
# Exercise 1: Conjugate gradient

Implement the linear conjugate gradient algorithm and compare it with one or more general optimisation methods (e.g. steepest descent, BFGS, ...). For example by plotting the gradient norm and/or error norm at each iteration.

The comparison could for example be done by considering a linear regression problem (e.g. `cars`) and/or a quadratic form and its corresponding formulation of solving a linear system of equations.

***
Lineære *conjugate gradient* er en iterativ metode til at løse et lineært ligningssystem
\begin{align*}
Ax = b.
\end{align*}
Dette svarer til at minimere følgende objektfunktion
\begin{align*}
 f(x)=\frac{1}{2}{x}^\top A {x}-{b}^\top {x},  
\end{align*}
idet 
\begin{align*}
    \nabla f( x) &= \frac{1}{2}2A{x}-{b}\\
    &= A{x}-{b}.
\end{align*}
Sættes gradienten lig nul, fås
\begin{align*}
    A{x} = {b}.
\end{align*}
Residualerne af det lineære system er givet ved
\begin{align*}
    {r}({x})=\nabla f({x})=A{x}-{b}.
\end{align*}

For at benyttes CG skal der bruges en mængde conjugate vektorer, $\{p_0, p_1, \ldots, p_{n-1}\}$, det vil sige at der gælder $p_i^\top A p_j = 0, \; \forall i \neq j$. Disse kan bestemmes på forskellige måder. Givet et startpunkt, $x_0$, og en mængden af conjugate directions vektorer, er iterationerne givet ved
\begin{align*}
    x_{k+1} = x_k + \alpha_k p_k,
\end{align*}
hvor skridtlængden, $\alpha_k$, er givet ved $\text{argmin} (f(x_k+\alpha p_k))$. $\alpha_k$ kan eksplicit skrives som 
\begin{align*}
    \alpha_k = - \frac{r_k^\top r_k}{p^\top_k A P_k}.
\end{align*}
En *"bilig"* metode til at vælge mængden af conjugate vektorer er ved brug af *conjugate gradiant*. Her benyttes kun den forrige conjugate vektor til at regne en ny conjugate vektor. I conjugate gradient er hvert $p_k$ valgt til at være en linearkombination af det negative residual og den forrige retning.
\begin{align*}
    p_k = -r_k + \beta_k p_{k-1},
\end{align*}
hvor 
\begin{align*}
    \beta_k = \frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k}.
\end{align*}
Det er muligt nemt, at optimere ovenstående, sådan at der er færre matrix-vektor produkter for hver iteration. Dette ses i følgende algoritme, hvor $p_0$ er valgt til at være steepest descent retningen.
```{r}
# Implementation af conjugate gradient
MinimizeCG52 <- function(x_k, A, b, tol, k_max) {
 r_k <- A %*% x_k - b
 p_k <- - r_k
 k   <- 0
 rTr <- t(r_k) %*% r_k
 while (norm(r_k , type = "2") > tol & k < k_max) { 
   rTr_old <- rTr
   alpha_k <- rTr_old / (t(p_k) %*% A %*% p_k)
   x_k     <- x_k + alpha_k[1] * p_k
   r_k     <- r_k + alpha_k[1] * A %*% p_k
   rTr     <- t(r_k) %*% r_k
   beta_k  <- rTr / rTr_old
   p_k     <- -r_k + beta_k[1] * p_k
   k       <- k + 1
 }
 sol <- list(par = x_k , iter = k)
 return(sol)
}
```
Implementerer BFGS og Steepest Descent for at sammenligne med disse.

```{r}
# Implementation af backtracking
backtracking_line_search <- function(alpha, c, rho, x_k, p_k, g_k, f) {
  alpha_k <- alpha
	repeat { 
		lhs <- f(x_k + alpha_k * p_k)
		rhs <- f(x_k) + c * alpha_k * g_k %*% p_k
		if (lhs <= rhs) break
		alpha_k <- rho * alpha_k
	}
	return(alpha_k)
}

# Implementation af BFGS
BFGS <- function(f, g, x_k, alpha, c, rho, tol, k_max){
  k <- 0
  n <- length(x_k)
  H_k <- I_n <- diag(n)
  g_k <- tol + 1
  while (norm(g_k, type="2") > tol & k < k_max) {
    g_k <- g(x_k)
    p_k <- -  H_k %*% g_k
    alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
    x_k <- x_k + alpha_k * p_k
    s_k <- alpha_k * p_k
    y_k <- g(x_k) - g_k
    rho_k <- 1 / (t(y_k) %*% s_k)[1]
    H_k <- (I_n - rho_k*s_k%*%t(y_k))%*%H_k%*%(I_n - rho_k*y_k%*%t(s_k)) +
      rho_k*s_k%*%t(s_k)
    k <- k + 1
  }
  sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k , inv_hess = H_k)
  return(sol)
}

# Implementerer Steepest descent
steepest_descent <- function(f, g, x_k, alpha, c, rho, tol, k_max) {
  k <- 0
	g_k <- tol + 1 # For at komme i gang
	
	while ((norm(g_k,type="2") > tol) & (k < k_max)) { # Type 2 er for vektorer
		g_k <- g(x_k)
		p_k <- -g_k 
		alpha_k <- backtracking_line_search(alpha , c, rho, x_k, p_k, g_k, f)
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
	}
	sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k)
	return(sol)
}
```

Tester implementation af Conjugate Gradient på cars datasættet og sammenligner med BFGS og Steepest Descent.

Der ønskes at lave en model af bremselængde som funktion af farten. Dette giver objektfunktionen
\[
f(x_1, x_2) = \frac{1}{n} \sum_{i = 1}^n (m(s_i) - d_i)^2,
\]
hvor $s$ er fart og $d$ er bremselængde og $m(s) = x_1 + x_2 \cdot s$. Der ønskes at bestemme $x_1$ og $x_2$ således, at objektfunktionen, $f$ minimeres.
```{r}
data(cars)

# Objektfunktionen defineres.
f <- function(x){
  n <- length(cars$dist)
  s <- cars$speed
  d <- cars$dist
  return(1/n * sum((x[1] + x[2] * s - d)^2))
}

# Gradienten defineres.
g <- function(x){
  n <- length(cars$speed)
  s <- cars$speed
  d <- cars$dist 
  g_x1 <- 1/n * sum(2 * (x[1] + x[2] * s - d) )
  g_x2 <- 1/n * sum(2 * (x[1] + x[2] * s - d) * s)
  return(c(g_x1,g_x2))
}

# For at opnå et lineært liningssystem, opskrives designmatricen.
X <-  matrix(1, nrow = length(cars$speed), ncol=2)
X[,2] <- cars$speed

# Opstiller udtrykkene for normalligningen til OLS problemmet.
A <- t(X) %*% X
b <- t(X) %*% cars$dist

# Vil nu løse A x_k = b med de forskellige metoder. Vælger værdier til funktionerne
x_k   <- c(-5,10)
alpha <- 1    
c     <- 1e-4
tol   <- 1e-4
rho   <- 0.5
k_max <- 10000


datCG   <- MinimizeCG52(x_k , A , b, tol, k_max)
datBFGS <- BFGS(f, g, x_k, alpha, c, rho, tol, k_max)
datSD   <- steepest_descent(f, g, x_k, alpha, c, rho, tol, k_max)
datlm   <- lm(cars$dist ~ cars$speed)  

frame <- data.frame(CG = c(datCG$iter,datCG$par) , BFGS = c(datBFGS$iter,datBFGS$par) ,
           SD = c(datSD$iter,datSD$par), LM = c(NA, coef(datlm)))
rownames(frame) <- c("Iter","x_1","x_2"); frame
```
Det ses at, *CG* benytter to iterationer, hvilket er forventeligt, idet dimensionen af problemet er $n = 2$. Dette er færre iterationer end de to andre metoder.

For at teste tidsforbruget for de tre metoder, anvendes funktionen "microbenchmark".
```{r}
microbenchmark(
  MinimizeCG52(x_k , A , b, tol, k_max),
  BFGS(f, g, x_k, alpha, c, rho, tol, k_max),
  steepest_descent(f, g, x_k, alpha, c, rho, tol, k_max),
  times = 10 , unit = "ms"
)
```
Det ses, at *CG* er den klart hurtigste af de tre metoder.
Kan gøres endnu hurtigere ved preconditioning.

***
# Exercise 2: Non-linear conjugate gradient

What are the differences between non-linear and linear conjugate gradient methods? 

Experiment with `R`'s implementation of conjugate gradient: `optim(par, fn, gr, method = "CG")` (see `?optim`). Use it to solve an optimisation problem.

***
En forskel på lineær *CG* og ikke-lineær *CG* er formen for $\alpha_k$. 

I det lineære tilfælde er $\alpha_k$ valgt som den værdi der minimerer den strengt konvekse og kvadratiske objektfunktion langs søgningsretningen $p_k$. Denne værdi er givet eksplicit. I det ikke-lineære tilfælde vælges $\alpha_k$ ligeledes som den værdi der approksimativt minimerer obejktfunktionen langs retningen $p_k$. Den største forskel er altså at at objektfunktionen ikke er lineær. For at bestemme skridtlængden $\alpha_k$ skal der gøres brug af en _line search algortime_.

I det ikke-lineære tilfælde er vi nødt til at kende noget mere specifikt om $\alpha_k$ for at sikre at $p_k$ opfylder at være en descent direction. Enten skal $\alpha$ være den eksakte minimizer for obejktfunktionen langs retningen $p_k$ ellers skal der pålægges at $\alpha$ skal overholde *Wolfe betingelserne*,
\begin{align*}
    f(x_k+\alpha_k p_k) &\leq f(x_k) + c_1 \alpha_k \nabla f_k^\top p_k,\\
    |\nabla f(x_k + \alpha_k p_k)^\top p_k| &\leq -c_2 \nabla f_k^\top p_k,
\end{align*}
hvor $0 < c_1 < c_2 < 1/2$. Begge vil sikre at $p_k$ er en descent direction.

En anden forskel på lineær og ikke-lineær *CG* er, at residualerne i det lineære tilfælde er givet ved gradienten til den strengt konvekse og kvadratiske objektfunktion hvorimod den i det ikke-lineære tilfæde er givet ved gradienten til den ikke-lineære objektfunktion.

Der gælder dermed at hvis den ikke lineære objektfunktion er strengt konveks og kvadratiske så er de to tilfælde ens.

Ovenstående *CG* algoritme kan derfor udvides til, at kunne benyttes for ikke-lineære funktioner hvis disse ændringer indføres. 

Yderligere gælder der for lineær *CG*, at den konvergerer efter højst $n$ iterationer eller højst efter antallet af forskellige egenværdier for koefficientmatricen, hvorimod den ikke-lineære *CG* har mere overraskende konvergensegenskaber. Det er ikke muligt at vise for den ikke-lineære *CG* at der konvergeres for $k \to \infty$, men det er tilgengæld muligt at vise at gradienten ikke er bundet væk fra nul, det vil sige det kan bevises, at
\begin{align*}
\liminf_{k \to \infty} \| \nabla f_k \| = 0.
\end{align*}


Prøver ikke-lineær *GC* på funktionen
\begin{align*}
  f(x_1, x_2) = x_1^2 + \frac{1}{4}x_2^2 + 4(x_1 - x_2)^2 \sin(x_2)^2,
\end{align*}
hvor gradienten er givet ved
\begin{align*}
  \nabla f(x_1, x_2) = \begin{pmatrix} 2x_1 +8(x_1 - x_2) \sin(x_2)^2 \\ \frac{1}{2}x_2 - 8(x_1 - x_2)^2 + 4(x_1 - x_2)^2 \sin(2x_2)\end{pmatrix}
\end{align*}
```{r}
# Definerer funktionen.
fn = function(x) x[1]^2 + .25*x[2]^2 + 4*(x[1] - x[2])^2*sin(x[2])^2

# Definerer gradienten.
gr = function(x) c(2*x[1] + 8*(x[1] - x[2])*sin(x[2])^2, .5*x[2] - 8*(x[1] 
                   -x[2])*sin(x[2])^2 + 4*(x[1] - x[2])^2*sin(2*x[2]))

# Vælger startpunkt
x_0 <- c(10,1)

# Benytter optim funktionen på f.
optim(x_0, fn, gr, method = "CG")
```

***

# Exercise 3: Conjugate gradient

When are conjugate gradient methods useful? How is the linear conjugate gradient different from for example solving the corresponding normal equations directly?

***

Den lineære _conjugate gradient_ metode, er god til løse problemer for store $n$.
I den implementerede *CG* algortime, skal der udføres ét matrix-vektor produkt, $Ap_k$, to gange indreprodukt, $p_k^\top(Ap_k)$ og $r_k^\top r$, og der skal udføres tre vektorsummeringer. Alle disse beregninger er med floating point, hvilket gør at der kan indgå fejl i beregningerne. Det kan derfor for mindre problemer være bedre at benytte QR- eller SVD-faktorisering, idet disse metoder ikke er så følsomme over for fejl fra floating point. Dog skal man i faktoriseringernen ændre på koefficientmatricen løbende, hvilket ikke gør sig gældende i \textit{CG}, derfor er det stadig bedre at anvende CG på store problemer. 