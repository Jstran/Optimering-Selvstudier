---
title: "Optimisation: Self study 5 (Conjugate gradient)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
printf <- function(...) cat(sprintf(...))
printfln <- function(...) cat(sprintf(...), "\n")
library(microbenchmark)
```
***
# Exercise 1: Conjugate gradient

Implement the linear conjugate gradient algorithm and compare it with one or more general optimisation methods (e.g. steepest descent, BFGS, ...). For example by plotting the gradient norm and/or error norm at each iteration.

The comparison could for example be done by considering a linear regression problem (e.g. `cars`) and/or a quadratic form and its corresponding formulation of solving a linear system of equations.

***
TEORI TEORI TEORI

```{r}
# Implementation af conjugate gradient
MinimizeCG52 <- function(x_k, A, b, tol, k_max) {
 r_k <- A %*% x_k - b
 p_k <- -r_k
 k   <- 0
 while (norm(r_k , type = "2") > tol & k < k_max) { 
   rTr     <- t(r_k) %*% r_k
   alpha_k <- rTr / (t(p_k) %*% A %*% p_k)
   x_k     <- x_k + alpha_k[1] * p_k
   r_old   <- r_k
   r_k     <- r_k + alpha_k[1] * A %*% p_k
   beta_k  <- (t(r_k) %*% r_k) / rTr
   p_k     <- -r_k + beta_k[1] * p_k
   k       <- k + 1
 }
 sol <- list(par = x_k , iter = k)
 return(sol)
}
```
Implementerer BFGS for at sammenligne med denne.

```{r}
# Implementation af backtracking
backtracking_line_search <- function(alpha, c, rho, x_k, p_k, g_k, f) {
  alpha_k <- alpha
	repeat { 
		lhs <- f(x_k + alpha_k * p_k)
		rhs <- f(x_k) + c * alpha_k * g_k %*% p_k
		if (lhs <= rhs) break
		alpha_k <- rho * alpha_k
	}
	return(alpha_k)
}

# Implementation af BFGS
BFGS <- function(f, g, x_k, alpha, c, rho, tol, k_max){
  k <- 0
  n <- length(x_k)
  H_k <- I_n <- diag(n)
  g_k <- tol + 1
  while (norm(g_k, type="2") > tol & k < k_max) {
    g_k <- g(x_k)
    p_k <- -  H_k %*% g_k
    alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
    x_k <- x_k + alpha_k * p_k
    s_k <- alpha_k * p_k
    y_k <- g(x_k) - g_k
    rho_k <- 1 / (t(y_k) %*% s_k)[1]
    H_k <- (I_n - rho_k*s_k%*%t(y_k))%*%H_k%*%(I_n - rho_k*y_k%*%t(s_k)) + rho_k*s_k%*%t(s_k)
    k <- k + 1
  }
  sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k , inv_hess = H_k)
  return(sol)
}

# Implementerer Steepest descent
steepest_descent <- function(f, g, x_k, alpha, c, rho, tol, k_max) {
  k <- 0
	g_k <- tol + 1 # For at komme i gang
	
	while ((norm(g_k,type="2") > tol) & (k < k_max)) { # Type 2 er for vektorer
		g_k <- g(x_k)
		p_k <- -g_k 
		alpha_k <- backtracking_line_search(alpha , c, rho, x_k, p_k, g_k, f)
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
	}
	sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k)
	return(sol)
}
```

Tester implementation af conjugate gradient på cars datasættet og sammenligner med BFGS og steepest descent.

MERE FYLDSGØRENDE TEKST
Der ønskes at lave en model af bremselængde som funktion af farten. Dette giver objektfunktionen 
\[
f(a, b) = \frac{1}{n} \sum_{i = 1}^n (m(s_i) - d_i)^2,
\]
```{r}
data(cars)

# Objektfunktionen
f <- function(ab){
  n <- length(cars$dist)
  s <- cars$speed
  d <- cars$dist
  return(1/n * sum((ab[1] + ab[2] * s - d)^2))
}
# Gradienten
g <- function(ab){
  n <- length(cars$speed)
  s <- cars$speed
  d <- cars$dist 
  g_a <- 1/n * sum(2 * (ab[1] + ab[2] * s - d) )
  g_b <- 1/n * sum(2 * (ab[1] + ab[2] * s - d) * s)
  return(c(g_a,g_b))
}

# Bestemmer designmatricen.
X <-  matrix(1, nrow = length(cars$speed), ncol=2)
X[,2] <- cars$speed

# Laver det om til et OLS problem.
A <- t(X) %*% X
b <- t(X) %*% cars$dist

# Vil nu løse A x_k = b med de forskellige metoder. Vælger værdier til funktionerne
x_k    <- c(-5,10)
alpha <- 1    
c <- 1e-4
tol <- 1e-4
rho <- 0.5
k_max <- 10000


MinimizeCG52(x_k , A , b, tol, k_max)$iter
BFGS(f, g, x_k, alpha, c, rho, tol, k_max)$iter
steepest_descent(f, g, x_k, alpha, c, rho, tol, k_max)$iter
```
NOGET TEKST DER BESKRIVER HVAD DER SKER.
Sammenligner tidsforbruget.
```{r}
microbenchmark(
  MinimizeCG52(x_k , A , b, tol, k_max)$iter,
  BFGS(f, g, x_k, alpha, c, rho, tol, k_max)$iter,
  steepest_descent(f, g, x_k, alpha, c, rho, tol, k_max)$iter,
  times = 10
)
```



***
# Exercise 2: Non-linear conjugate gradient

What are the differences between non-linear and linear conjugate gradient methods? 

Experiment with `R`'s implementation of conjugate gradient: `optim(par, fn, gr, method = "CG")` (see `?optim`). Use it to solve an optimisation problem.

***

***

# Exercise 3: Conjugate gradient

When are conjugate gradient methods useful? How is the linear conjugate gradient different from for example solving the corresponding normal equations directly?

***