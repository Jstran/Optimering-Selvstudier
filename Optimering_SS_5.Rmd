---
title: "Optimisation: Self study 5 (Conjugate gradient)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
printf <- function(...) cat(sprintf(...))
printfln <- function(...) cat(sprintf(...), "\n")
library(microbenchmark)
```
***
# Exercise 1: Conjugate gradient

Implement the linear conjugate gradient algorithm and compare it with one or more general optimisation methods (e.g. steepest descent, BFGS, ...). For example by plotting the gradient norm and/or error norm at each iteration.

The comparison could for example be done by considering a linear regression problem (e.g. `cars`) and/or a quadratic form and its corresponding formulation of solving a linear system of equations.

***
Lineære *conjugate gradient* er en iterativ metode til at løse et lineært ligningssystem
\begin{align*}
Ax = b.
\end{align*}
Dette svarer til at minimere følgende objektfunktion
\begin{align*}
 f(x)=\frac{1}{2}{x}^\top A {x}-{b}^\top {x},  
\end{align*}
idet 
\begin{align*}
    \nabla f( x) &= \frac{1}{2}2A{x}-{b}\\
    &= A{x}-{b}.
\end{align*}
Sættes gradienten lig nul, fås
\begin{align*}
    A{x} = {b}.
\end{align*}
Residualerne af det lineære system er givet ved
\begin{align*}
    {r}({x})=\nabla f({x})=A{x}-{b}.
\end{align*}

For at benyttes CG skal der bruges en mængde conjugate vektorer, $\{p_0, p_1, \ldots, p_{n-1}\}$, det vil sige at der gælder $p_i^\top A p_j = 0, \; \forall i \neq j$. Disse kan bestemmes på forskellige måder. Givet et startpunkt, $x_0$, og en mængden af conjugate directions vektorer, er iterationerne givet ved
\begin{align*}
    x_{k+1} = x_k + \alpha_k p_k,
\end{align*}
hvor skridtlængden, $\alpha_k$, er givet ved $\text{argmin} (f(x_k+\alpha p_k))$. $\alpha_k$ kan eksplicit skrives som 
\begin{align*}
    \alpha_k = - \frac{r_k^\top p_k}{p^\top_k A P_k}.
\end{align*}
En *"bilig"* metode til at vælge mængden af conjugate vektorer er ved brug af *conjugate gradiant*. Her benyttes kun den forrige conjugate vektor til at regne en ny conjugate vektor. I conjugate gradient er hvert $p_k$ valgt til at være en linearkombination af det negative residual og den forrige retning.
\begin{align*}
    p_k = -r_k + \beta_k p_{k-1},
\end{align*}
hvor 
\begin{align*}
    \beta_k = \frac{r_k^\top A p_{k-1}}{p_{k-1}^\top A p_{k-1}}.
\end{align*}
Det er muligt nemt, at optimere ovenstående, sådan at der er færre matrix-vektor produkter for hver iteration. Dette ses i følgende algoritme, hvor $p_0$ er valgt til at være steepest descent retningen.
```{r}
# Implementation af conjugate gradient
MinimizeCG52 <- function(x_k, A, b, tol, k_max) {
 r_k <- A %*% x_k - b
 p_k <- - r_k
 k   <- 0
 rTr <- t(r_k) %*% r_k
 while (norm(r_k , type = "2") > tol & k < k_max) { 
   rTr_old <- rTr
   alpha_k <- rTr_old / (t(p_k) %*% A %*% p_k)
   x_k     <- x_k + alpha_k[1] * p_k
   r_k     <- r_k + alpha_k[1] * A %*% p_k
   rTr     <- t(r_k) %*% r_k
   beta_k  <- rTr / rTr_old
   p_k     <- -r_k + beta_k[1] * p_k
   k       <- k + 1
 }
 sol <- list(par = x_k , iter = k)
 return(sol)
}
```
Implementerer BFGS og Steepest Descent for at sammenligne med disse.

```{r}
# Implementation af backtracking
backtracking_line_search <- function(alpha, c, rho, x_k, p_k, g_k, f) {
  alpha_k <- alpha
	repeat { 
		lhs <- f(x_k + alpha_k * p_k)
		rhs <- f(x_k) + c * alpha_k * g_k %*% p_k
		if (lhs <= rhs) break
		alpha_k <- rho * alpha_k
	}
	return(alpha_k)
}

# Implementation af BFGS
BFGS <- function(f, g, x_k, alpha, c, rho, tol, k_max){
  k <- 0
  n <- length(x_k)
  H_k <- I_n <- diag(n)
  g_k <- tol + 1
  while (norm(g_k, type="2") > tol & k < k_max) {
    g_k <- g(x_k)
    p_k <- -  H_k %*% g_k
    alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
    x_k <- x_k + alpha_k * p_k
    s_k <- alpha_k * p_k
    y_k <- g(x_k) - g_k
    rho_k <- 1 / (t(y_k) %*% s_k)[1]
    H_k <- (I_n - rho_k*s_k%*%t(y_k))%*%H_k%*%(I_n - rho_k*y_k%*%t(s_k)) + rho_k*s_k%*%t(s_k)
    k <- k + 1
  }
  sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k , inv_hess = H_k)
  return(sol)
}

# Implementerer Steepest descent
steepest_descent <- function(f, g, x_k, alpha, c, rho, tol, k_max) {
  k <- 0
	g_k <- tol + 1 # For at komme i gang
	
	while ((norm(g_k,type="2") > tol) & (k < k_max)) { # Type 2 er for vektorer
		g_k <- g(x_k)
		p_k <- -g_k 
		alpha_k <- backtracking_line_search(alpha , c, rho, x_k, p_k, g_k, f)
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
	}
	sol <- list(par = x_k , func = f(x_k) , grad = g_k , iter = k)
	return(sol)
}
```

Tester implementation af Conjugate Gradient på cars datasættet og sammenligner med BFGS og Steepest Descent.

Der ønskes at lave en model af bremselængde som funktion af farten. Dette giver objektfunktionen
\[
f(x_1, x_2) = \frac{1}{n} \sum_{i = 1}^n (m(s_i) - d_i)^2,
\]
hvor $s$ er fart og $d$ er bremselængde og $m(s) = x_1 + x_2 \cdot s$. Der ønskes at bestemme $x_1$ og $x_2$ således, at objektfunktionen, $f$ minimeres.
```{r}
data(cars)

# Objektfunktionen defineres.
f <- function(x){
  n <- length(cars$dist)
  s <- cars$speed
  d <- cars$dist
  return(1/n * sum((x[1] + x[2] * s - d)^2))
}

# Gradienten defineres.
g <- function(x){
  n <- length(cars$speed)
  s <- cars$speed
  d <- cars$dist 
  g_x1 <- 1/n * sum(2 * (x[1] + x[2] * s - d) )
  g_x2 <- 1/n * sum(2 * (x[1] + x[2] * s - d) * s)
  return(c(g_x1,g_x2))
}

# For at opnå et lineært liningssystem, opskrives designmatricen.
X <-  matrix(1, nrow = length(cars$speed), ncol=2)
X[,2] <- cars$speed

# Opstiller udtrykkene for normalligningen til OLS problemmet.
A <- t(X) %*% X
b <- t(X) %*% cars$dist

# Vil nu løse A x_k = b med de forskellige metoder. Vælger værdier til funktionerne
x_k   <- c(-5,10)
alpha <- 1    
c     <- 1e-4
tol   <- 1e-4
rho   <- 0.5
k_max <- 10000


datCG   <- MinimizeCG52(x_k , A , b, tol, k_max)
datBFGS <- BFGS(f, g, x_k, alpha, c, rho, tol, k_max)
datSD   <- steepest_descent(f, g, x_k, alpha, c, rho, tol, k_max)
datlm   <- lm(cars$dist ~ cars$speed)  

frame <- data.frame(CG = c(datCG$iter,datCG$par) , BFGS = c(datBFGS$iter,datBFGS$par) ,
           SD = c(datSD$iter,datSD$par), LM = c(NA, coef(datlm)))
rownames(frame) <- c("Iter","x_1","x_2"); frame
```
Det ses at, *CG* benytter to iterationer, hvilket er forventeligt, idet dimensionen af problemet er $n = 2$. Dette er færre iterationer end de to andre metoder.

For at teste tidsforbruget for de tre metoder, anvendes funktionen "microbenchmark".
```{r}
microbenchmark(
  MinimizeCG52(x_k , A , b, tol, k_max),
  BFGS(f, g, x_k, alpha, c, rho, tol, k_max),
  steepest_descent(f, g, x_k, alpha, c, rho, tol, k_max),
  times = 1 , unit = "ms"
)
```
Det ses, at *CG* er den klart hurtigste af de tre metoder.

***
# Exercise 2: Non-linear conjugate gradient

What are the differences between non-linear and linear conjugate gradient methods? 

Experiment with `R`'s implementation of conjugate gradient: `optim(par, fn, gr, method = "CG")` (see `?optim`). Use it to solve an optimisation problem.

***
Den væsentligste forskel på lineær *CG* og ikke-lineær *CG* er formen for $\alpha_k$ og $\beta_k$. Ovenstående *CG* algoritme kan derfor udvides til, at kunne benyttes for ikke-lineære funktioner. 


I det lineære tilfælde er $\alpha_k$ valgt som den værdi der minimerer den lineære objektfunktion langs søgningsretningen $p_k$. I det ikke-lineære tilfælde vælges $\alpha_k$ ligeledes som den værdi der approksimativt minimerer obejktfunktionen langs retningen $p_k$. Den største forskel er altså at at objektfunktionen ikke er lineær. Fpr at bestemme skridtlængden $\alpah_k$ skal der gøres brug af en _line search algortime_  

***

# Exercise 3: Conjugate gradient

When are conjugate gradient methods useful? How is the linear conjugate gradient different from for example solving the corresponding normal equations directly?

***