---
title: "Optimisation: Self study 2"
output: html_document
---
\usepackage{amsmath}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exercise 1

We have considered forward-difference (using $f(x)$ and $f(x + h)$) and central-difference (using $f(x - h)$ and $f(x + h)$). 

1. What would happen if we extend the central-difference to also use $f(x - 2h)$ and $f(x + 2h)$? Hint: consider the Taylor series up to a sufficiently high power of $h$. Hint: "five-point stencil" (<- På wiki :-) ).

Forward difference (FD) er givet ved
\begin{align*}
f'(x)\approx\frac{f(x+h) - f(x)}{h}.
\end{align*}
Kaldes Backward difference hvis første led i tælleren er f(x-h).

Central difference (CD) er givet ved
\begin{align*}
f'(x)\approx\frac{f(x+h) - f(x-h)}{2h}.
\end{align*}
Taylors formel er givet ved
\begin{align*}
f(x) = \sum^\infty_{j=0}\frac{f^{(j)}(x_0)}{j!}(x-x_0)^j
\end{align*}
og benyttes til at approximere funktionerne $f(x\pm h)$ og $f(x\pm 2h)$.
\begin{align*}
  f(x+h)  &= f(x) + hf'(x) + \frac{1}{2}h^2f''(x) 
            + \frac{1}{6}h^3f^{(3)}(x) + \frac{1}{24}h^4f^{(4)}(x)                 + \frac{1}{120}h^5f^{(5)}(x)+O(h^6)\\
  f(x-h)  &= f(x) - hf'(x) + \frac{1}{2}h^2f''(x) 
            - \frac{1}{6}h^3f^{(3)}(x) + \frac{1}{24}h^4f^{(4)}(x) 
            - \frac{1}{120}h^5f^{(5)}(x)+O(h^6)\\
  f(x+2h) &= f(x) + 2hf'(x) +2h^2f''(x) + \frac{4}{3}h^3f^{(3)}(x)
            + \frac{2}{3}h^4f^{(4)}(x) + \frac{4}{15}h^5f^{(5)}(x)
            + O(h^6)\\
  f(x-2h) &= f(x) - 2hf'(x) +2h^2f''(x) - \frac{4}{3}h^3f^{(3)}(x)
            + \frac{2}{3}h^4f^{(4)}(x) - \frac{4}{15}h^5f^{(5)}(x)
            + O(h^6)\\
\end{align*}
Bestemmer $D_1 = f(x+h) - f(x-h)$ og $D_2 = f(x+2h) - f(x-2h)$ (Op til $O(h^4)$).
\begin{align*}
  D_1 &= 2hf'(x) + \frac{1}{3}h^3f^{(3)}(x) + O(h^4)\\
  D_2 &= 4hf'(x) + \frac{8}{3}h^3f^{(3)}(x) + O(h^4)
\end{align*}
For at eliminere leddene med $h^3$, bestemmes $8D_1-D_2$ og $f'(x)$ isoleres.
\begin{align*}
  8D_1 - D_2 &= 12hf'(x) + O(h^4)\\
  &\Updownarrow\\
  f'(x) &= \frac{8D_1 - D_2}{12h} - O(h^3)\\[2mm]
  &= \frac{8( f(x+h) - f(x-h) ) - ( f(x+2h) - f(x-2h) )}{12h} - O(h^3)
\end{align*}

2. Analyse this method in comparison with FD and CD (theoretically and practically on specific examples).
3. What are the advantages and disadvantages of the different finite difference methods?

# Exercise 2

Implement algorithmic differentiation (AD) for univariate ($\mathbb{R} \to \mathbb{R}$) functions in `R` (supporting the following operations: `+`, `-`, `*`, `/`, `sin`, `cos`, `exp`). Use on the following problem and compare it with other ways of calculating the derivatives:

\[
  f(x) = \cos[ \sin(x) \cos(x) ]
\]

Extend your implementation to handle multivariate ($\mathbb{R} \to \mathbb{R}$) functions and use on the following problem and compare it with other ways of calculating the derivatives:

\[
  f(x) = [ x_1 x_2 \sin(x_3) + \exp(x_1 x_2) ] / x_3 \tag{8.26}
\]

# Exercise 3

In a gradient descent problem (e.g. Rosenbrock's function or best straight line for `cars` dataset), compare the use of exact and numerical derivatives and discuss it. The comparisons can include e.g. illustrations or summary measures (number of iterations, amount of time spent, accuracy of solution and possibly other aspects).

Remember that in `R`, there are many ways of registering amount of time spent. For very fast operations, you can use:

```{r}
library(microbenchmark)
X <- model.matrix(~ speed, cars)
microbenchmark(lm(dist ~ speed, cars), 
               lm.fit(X, cars$dist),
               lm.fit(model.matrix(~ speed, cars), cars$dist), 
               times = 100)
```

For slower operations, you can do it manually:

```{r}
time_begin <- proc.time()

for (i in 1:1000) {
  lm(dist ~ speed, cars)
}

time_end <- proc.time()
time_duration <- time_end - time_begin
time_duration_secs <- time_duration["user.self"]
time_duration_secs
```


# Exercise 4: Be creative!

If you have anything, put it here.

