---
title: 'Optimisation: Self study 2'
output:
  html_document: default
  pdf_document: default
---
\usepackage{amsmath}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("microbenchmark")
printf <- function(...) cat(sprintf(...))
printfln <- function(...) cat(sprintf(...), "\n")
Outer <- function(x,y,fun) {
   mat <- matrix(NA, length(x), length(y))
   for (i in seq_along(x)) {
            for (j in seq_along(y)) {mat[i,j] <- fun(x[i],y[j])} }
   mat}
```

# Exercise 1

We have considered forward-difference (using $f(x)$ and $f(x + h)$) and central-difference (using $f(x - h)$ and $f(x + h)$). 

***
1. What would happen if we extend the central-difference to also use $f(x - 2h)$ and $f(x + 2h)$? Hint: consider the Taylor series up to a sufficiently high power of $h$. Hint: "five-point stencil".

***
Taylors formel er givet ved
\begin{align*}
  f(x) = f(x) + h f'(x) + \frac{1}{2!} h^2 f''(x) + \frac{1}{3!} h^3 f^{(3)}(x) + \cdots.
\end{align*}

Forward difference (FD) er givet ved
\begin{align*}
f'(x)\approx\frac{f(x+h) - f(x)}{h}.
\end{align*}
Kaldes Backward difference hvis første led i tælleren er $f(x-h)$.

Central difference (CD) er givet ved
\begin{align*}
f'(x)\approx\frac{f(x+h) - f(x-h)}{2h}.
\end{align*}

Taylors formel benyttes til at approximere funktionerne $f(x\pm h)$ og $f(x\pm 2h)$.
\begin{align*}
  f(x\pm h)  &= f(x) \pm hf'(x) + \frac{1}{2}h^2f''(x) 
            \pm \frac{1}{6}h^3f^{(3)}(x) + O(h^4)\\
  f(x\pm 2h) &= f(x) \pm 2hf'(x) +2h^2f''(x)
            \pm \frac{4}{3}h^3f^{(3)}(x) + O(h^4)
\end{align*}
Bestemmer $D_1 = f(x+h) - f(x-h)$ og $D_2 = f(x+2h) - f(x-2h)$.
\begin{align*}
  D_1 &= 2hf'(x) + \frac{1}{3}h^3f^{(3)}(x) + O(h^4)\\
  D_2 &= 4hf'(x) + \frac{8}{3}h^3f^{(3)}(x) + O(h^4)
\end{align*}
For at eliminere leddene med $h^3$, bestemmes $8D_1-D_2$ og $f'(x)$ isoleres.
\begin{align*}
  8D_1 - D_2 &= 12hf'(x) + O(h^4)\\
  &\Updownarrow\\
  f'(x) &= \frac{8D_1 - D_2}{12h} - O(h^3)\\[2mm]
  &= \frac{8( f(x+h) - f(x-h) ) - ( f(x+2h) - f(x-2h) )}{12h} - O(h^3)
\end{align*}

***
2. Analyse this method in comparison with FD and CD (theoretically and practically on specific examples).

***

For at sammenligne de forskellige metoder, betragtes de tilhørende trunkeringsfejl.

For at bestemme trunkleringsfejlen for \textit{forward difference} betragtes Taylorudvidelsen hvor $f'(x)$ er isoleret:
\begin{align*}
f'(x)=\frac{f(x+h) - f(x)}{h} - \frac{1}{2} h f''(x) - \frac{1}{6} h^2 f^{(3)}(x) - O(h^3).
\end{align*}

Trunkeringsfejlen som en funktion af $h$ er givet ved
$$c_1 h + c_2 h^2 + \cdots.$$ 
Idet vi er interesseret i $h \to 0$. Specielt gælder der for $h < 1$ at, $$h > h^2 > h^3 > \cdots.$$
Dette betyder at trunkeringsfejlen er givet ved
\begin{align*}
    O(h)
\end{align*}

Trunkeringsfejlene for de forskellige metoder er givet ved. 

- FD er $O(h)$
- CD er $O(h^2)$
- Udvidet CD er $O(h^3)$


**Afrundingsfejl**

Betragtes forward difference 
\begin{align*}
    f'(x)\approx \frac{f(x+h)-f(x)}{h},
\end{align*}
hvor det antages at $x$ og $(x+h)$ er udregnet eksakte. 
Ved at evaluere $f(x)$ vil der komme en afrundingsfejl (round-off error) og dermed fås
\begin{align*}
    \frac{f(x+h)(1 + \delta_1)-f(x)(1 + \delta_2)}{h} = \frac{f(x+h)-f(x)}{h}+\frac{\delta_1f(x+h)-\delta_2f(x)}{h},
\end{align*}
hvor $\delta_1f(x+h)$ angiver afrundingsfejlen forbundet med evelueringen af $f(x+h)$, og $\delta_2f(x)$ angiver afrundingsfejlen forbundet med evelueringen af $f(x)$ og dermed angiver den sidste brøk den samlede afrundsingsfejl for forward difference.

Groft estimeret er $|\delta_i|\leq \varepsilon_M$ for $i \in \{1,2\}$, hvor $\varepsilon_M$ er machine epsilon. Ved omskrivning haves det, at afrundingsfejlen er givet ved
\begin{align*}
    \frac{\delta_1 f(x + h) - \delta_2 f(x)}{h} & \leq \frac{| \delta_1 f(x + h) - \delta_2 f(x) |}{h}\\
     &\leq \frac{| \delta_1 f(x + h) | + | -\delta_2 f(x) |}{h}\\
     &= \frac{| \delta_1 | | f(x + h) | + | \delta_2 | | f(x) |}{h}\\
     &\leq \frac{\varepsilon_M(| f(x + h) | + |f(x) |)}{h}.
\end{align*}
Dette medfører at afrundingsfejlen er $O(\frac{\varepsilon_M}{h})$.

Det bemærkes at denne samme afrudningsfejl gør sig gældende for de to andre metoder (CD og udvidet CD).

Det ses at for $h \to 0$ vil afrundingsfejlen blive større og større. Det betyder, at størrelsen på $h$ har den modsatteeffekt i forhold til trunkeringsfejlen. Der skal altså betragtes et trade-off mellem størrelsen af trunkeringsfejlen og størrelsen af afrundingsfejlen i forhold til størrelsen af $h$, idet summen af de to fejltyper tilsammen giver den totale fejl. 


**Funktionsevalueringer**

For at sammenligne antal funktionsevalueringer opskrives de forskellige metoder for $x \in \mathbb R^n$.

Forward difference:
\begin{align*}
\frac{\partial f}{\partial x_i}(x)\approx\frac{f(x +h e_i) - f(x)}{h e_i}, \quad \text{for }i=1,\ldots,n.
\end{align*}

Central difference:
\begin{align*}
\frac{\partial f}{\partial x_i}(x)\approx\frac{f(x + h e_i) - f(x - h e_i)}{h e_i}, \quad \text{for }i=1,\ldots,n.
\end{align*}

Udvidet central difference:
\begin{align*}
\frac{\partial f}{\partial x_i}(x)\approx\frac{8( f(x + h e_i) - f(x- h e_i) ) - ( f(x+2 h e_i) - f(x-2 h e_i) )}{12 h e_i}, \quad\text{for }i=1,\ldots,n
\end{align*}

Metoderne kræver følgende antal funktionsevalueringer

- FD kræver $n+1$ funktionsevalueringer
- CD kræver $2n$ funktionsevalueringer
- Udvidet CD kræver $4n$ funktionsevalueringer


**Praktisk eksempel for sammenligning af metoderne**

Anvender det forskellige metoder på funktionen
\begin{align*}
f(x)=\cos(\cos(x)\sin(x)).
\end{align*}

```{r}
# Vælger værdier
eps <- .Machine$double.eps
x <- pi/3

# Definerer funktioner 
f        <- function(x){cos(cos(x)*sin(x))}
df.exact <- function(x){-cos(2*x)*sin( cos(x)*sin(x) )}
FD       <- function(x,h){(f(x+h) - f(x))/h}
CD       <- function(x,h){(f(x+h) - f(x-h))/(2*h)}
ExpCD    <- function(x,h){(8*( f(x+h) - f(x-h) )
                          - ( f(x+2*h) - f(x-2*h) ))/(12*h)}

# For-løkke til evaluering af metoderne
FD_val <- CD_val <- ExpCD_val <- h <- numeric(16)
for (i in seq(1,16, length.out = 16)){
h[i] <- 10^{-i}
FD_val[i]    <- FD(x,h[i])
CD_val[i]    <- CD(x,h[i])
ExpCD_val[i] <- ExpCD(x,h[i])
}

# Afvigelse mellem approksimater og den eksakte afledte.
FD_err    <- abs(FD_val - df.exact(x))
CD_err    <- abs(CD_val - df.exact(x))
ExpCD_err <- abs(ExpCD_val - df.exact(x))

# Potter afvigelser mod h værdier.
plot(log10(h) , log10( ExpCD_err ) , 
     type = "l" , col = "green" , xlab = "log10(h)" , ylab="log10(Afvigelse)")
lines(log10(h) , log10( CD_err ) , col = "blue")
lines(log10(h) , log10( FD_err ) , col = "red")

legend(-16, -9.5, legend=c("FD", "CD" , "ExpCD"),
       col=c("red", "blue" , "green" ), lty=1, cex=0.8)

```

Det ses at for tilpas store $h$ (ca. $h=10^{-6}$), vil den udvidede CD være et bedre approksimat af $f'(x)$ end CD og FD, idet afvigelsen for disse værdier af $h$, er lavere end CD og FD (se grøn kurve). For dette eksempel (Dim = 1) ses det at ExpCD kræver 4 funktionsevalueringer, hvilket er dobbelt så mange som for både FD og CD. 


***
3. What are the advantages and disadvantages of the different finite difference methods?

***

(Denne opgave er skrevet ind i opgave 1.2)


# Exercise 2

***
Implement algorithmic differentiation (AD) for univariate ($\mathbb{R} \to \mathbb{R}$) functions in `R` (supporting the following operations: `+`, `-`, `*`, `/`, `sin`, `cos`, `exp`). Use on the following problem and compare it with other ways of calculating the derivatives:

\[
  f(x) = \cos[ \sin(x) \cos(x) ]
\]

***
Algortimisk differentiation er en måde at udregne afledt på, baseret på at kunne opdele funktioner i sekvenser af elementære regneoperationer. I det følgende implementeres en række operationer ud fra denne metode.
```{r}
# Definerer hvad et "ADnum" er.
create_ADnum = function(val, deriv = 1) {
x = list(val = val, deriv = deriv)
class(x) = "ADnum"
return(x)
}

# Definerer hvad der forstås ved at printe et ADnum
print.ADnum = function(x, ...) {
cat("value = ", x$val,
" and deriv = ", x$deriv, "\n", sep = "")
return(invisible(x))
}
     
#Definerer produktreglen og brøkreglen
Ops.ADnum = function(e1, e2) {
# LHS constant, e.g. 2*x: e1 is a number 2, convert to ADnum
if (.Method[1] == "") {
e1 = create_ADnum(e1, 0)
}

# RHS constant, e.g. x*2: e2 is a number 2, convert to ADnum
if (.Method[2] == "") {
e2 = create_ADnum(e2, 0)
}

# Definerer hvordan man adderer ADnums
if (.Generic == "+") {
return(create_ADnum(e1$val + e2$val, e1$deriv + e2$deriv) )
}
  
# Definerer hvordan man subtraherer ADnums
if (.Generic == "-") {
return(create_ADnum(e1$val - e2$val, e1$deriv - e2$deriv) )
}
  
# Definerer hvordan man ganger ADnums (Produktregel) 
if (.Generic == "*") {
return(create_ADnum(e1$val * e2$val, e1$deriv*e2$val + e1$val*e2$deriv) )
}

# Definerer hvordan man dividerer ADnums (Brøkregel)  
if (.Generic == "/") {
return(create_ADnum(e1$val / e2$val, (e1$deriv*e2$val - e1$val*e2$deriv)/e2$val^2) )
}
  
stop("Function ’", .Generic, "’ not yet implemented for ADnum")
}

#Definerer cos, sin og exp
Math.ADnum = function(x, ...) {
if (.Generic == "cos") {
  return(create_ADnum(cos(x$val), -sin(x$val)*x$deriv))
} 
else if (.Generic == "sin") {
  return(create_ADnum(sin(x$val), cos(x$val)*x$deriv))
} 
else if (.Generic == "exp") {
  return(create_ADnum(exp(x$val), exp(x$val)*x$deriv))
}

stop("Function ’", .Generic, "’ not yet implemented for ADnum")
}
```

**Forward mode**

Her evalueres en retningsafledt, for hvert $x_i$ i en given retning $p\in \mathbb{R}$. (Det bemærkes at her huskes der både på evalueringen af den retningsafledte og evalueringen af $x_i$).

$$D_px_i=\nabla x_i^\top p = \sum_{j=1}^n \frac{\partial x_i}{\partial x_k}p_j, \quad i=1,\dots k,$$

hvor $p$ er en \textit{"seed" vektor} og $n$ angiver antal startinput.

**Praktisk anvendelse**

Ved at betragte den givne funktion $f$, kan den inddeles i følgende elementære regneoperationer

\begin{align*}
    v_0 &= x\\
    v_1 &= \cos(v_0)\\
    v_2 &= \sin(v_0)\\
    v_3 &= v_1 \cdot v_2\\
    v_4 &= \cos(v_3).
\end{align*}

Ved brug af **forward mode** opnås tabel i noter.

Anvender metoderne _AD_, _FD_, _CD_ og _ExpCD_ på funktionen $f$.

```{r}
# Definerer funktion og gradient.
f <- function(x) cos(sin(x)*cos(x))
g_exact <- function(x) -(cos(x)^2-sin(x)^2)*sin(sin(x)*cos(x))

# Vælger værdier.
x <- pi/2
h <- 10^-5

# Definerer xAD til at være et AD_num.
xAD <- create_ADnum(x)

frame <- data.frame(Exact = c(f(x),g_exact(x)) , AD = c(f(xAD)$val,f(xAD)$deriv) , FD = c(NA,FD(x,h)) , CD = c(NA,CD(x,h)) , ExpCD = c(NA,ExpCD(x,h)) )
rownames(frame) <- c("Function","Gradient") ; frame

```

# Exercise 3

***
In a gradient descent problem (e.g. Rosenbrock's function or best straight line for `cars` dataset), compare the use of exact and numerical derivatives and discuss it. The comparisons can include e.g. illustrations or summary measures (number of iterations, amount of time spent, accuracy of solution and possibly other aspects).

***

Remember that in `R`, there are many ways of registering amount of time spent. For very fast operations, you can use:

```{r}
library(microbenchmark)
X <- model.matrix(~ speed, cars)
microbenchmark(lm(dist ~ speed, cars), 
               lm.fit(X, cars$dist),
               lm.fit(model.matrix(~ speed, cars), cars$dist), 
               times = 100)
```

For slower operations, you can do it manually:

```{r}
time_begin <- proc.time()

for (i in 1:1000) {
  lm(dist ~ speed, cars)
}

time_end <- proc.time()
time_duration <- time_end - time_begin
time_duration_secs <- time_duration["user.self"]
time_duration_secs
```



```{r}

f <- function(x) (5*x[1]^2 + x[2]^2)/2
g <- function(x) c(5*x[1],x[2])

FDm <- function(x,h = 1e-4){
  n <- length(x)
  grad <- numeric(n)
  
  for ( i in 1:n ){
    e <- tabulate(i, nbins = n)
    grad[i] <- (f(x+e*h) - f(x))/h
  }
  return(grad)
}

CDm <- function(x,h = 1e-4){
  n <- length(x)
  grad <- numeric(n)
  
  for ( i in 1:n ){
    e <- tabulate(i, nbins = n)
    grad[i] <- (f(x+e*h) - f(x-e*h))/(2*h)
  }
  return(grad)
}

ExpCDm <- function(x,h = 1e-4){
  n <- length(x)
  grad <- numeric(n)
  
  for ( i in 1:n ){
    e <- tabulate(i, nbins = n)
    grad[i] <- (8*(f(x+e*h) - f(x-e*h)) - ( f(x+2*e*h) - f(x-2*e*h) ))/(12*h)
  }
  return(grad)
}


backtracking_line_search <- function(alpha, c, rho, x_k, p_k, g_k, f) {
	alpha_k <- alpha
	repeat { 
		lhs <- f(x_k + alpha_k * p_k)
		rhs <- f(x_k) + c * alpha_k * g_k %*% p_k
		if (lhs <= rhs) break
		alpha_k <- rho * alpha_k
	}
	return(alpha_k)
}

f_xy <- function(x,y) f(c(x,y))
N <- 50
x <- y <- seq(-2,2,length.out = N) 
z <- Outer(x,y,f_xy)
lev <- c(.1,.5,1,2.5,5,10)

steepest_descent <- function(f, x_k, alpha, c, rho, tol, k_max,FUN) {
  k <- 0
	g_k <- tol + 1 # For at komme i gang
	contour(x, y, z, levels = lev)
	
	while ((norm(g_k,type="2") > tol) & (k < k_max)) { # Type 2 er for vektorer
		g_k <- FUN(x_k)
		p_k <- -g_k 
		alpha_k <- backtracking_line_search(alpha , c, rho, x_k, p_k, g_k, f)
		x_old <- x_k
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
		
		lines(c(x_old[1],x_k[1]) , c(x_old[2],x_k[2]) , col = "red")
		
	}
	printfln("k = %d \t x_k = (%6.3f , %6.3f) \t f_k = %.2f \t g_k = (%8.3f , %8.3f) \t ", k, x_k[1] , x_k[2], f(x_k), g_k[1] , g_k[2])
	
	
	return(x_k)
}
```

```{r}
x_k <- c(1,1)
alpha <- 0.3
c <- tol <- 1e-4
rho <- 0.5
k_max <- 1000

steepest_descent(f , x_k , alpha , c , rho , tol , k_max , g)
steepest_descent(f, x_k , alpha , c , rho , tol , k_max ,FDm)
steepest_descent(f, x_k , alpha , c , rho , tol , k_max ,ExpCDm)
```

```{r}
steepest_descentnp <- function(f, x_k, alpha, c, rho, tol, k_max,FUN) {
  k <- 0
	g_k <- tol + 1 # For at komme i gang
	
	while ((norm(g_k,type="2") > tol) & (k < k_max)) { # Type 2 er for vektorer
		g_k <- FUN(x_k)
		p_k <- -g_k 
		alpha_k <- backtracking_line_search(alpha , c, rho, x_k, p_k, g_k, f)
		x_old <- x_k
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
	}
	
	return(x_k)
}

microbenchmark(
steepest_descentnp(f, x_k , alpha , c , rho , tol , k_max , g) ,
steepest_descentnp(f, x_k , alpha , c , rho , tol , k_max , FDm) ,
steepest_descentnp(f, x_k , alpha , c , rho , tol , k_max , ExpCDm) ,
unit = "ms",
times = 100
)
```
