---
title: "Optimisation: Self study 2"
output: html_document
---
\usepackage{amsmath}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("microbenchmark")
```

# Exercise 1

We have considered forward-difference (using $f(x)$ and $f(x + h)$) and central-difference (using $f(x - h)$ and $f(x + h)$). 

***
1. What would happen if we extend the central-difference to also use $f(x - 2h)$ and $f(x + 2h)$? Hint: consider the Taylor series up to a sufficiently high power of $h$. Hint: "five-point stencil".

***

Forward difference (FD) er givet ved
\begin{align*}
f'(x)\approx\frac{f(x+h) - f(x)}{h}.
\end{align*}
Kaldes Backward difference hvis første led i tælleren er f(x-h).

Central difference (CD) er givet ved
\begin{align*}
f'(x)\approx\frac{f(x+h) - f(x-h)}{2h}.
\end{align*}
Taylors formel er givet ved
\begin{align*}
f(x) = \sum^\infty_{n=0}\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n
\end{align*}
og benyttes til at approximere funktionerne $f(x\pm h)$ og $f(x\pm 2h)$.
\begin{align*}
  f(x\pm h)  &= f(x) \pm hf'(x) + \frac{1}{2}h^2f''(x) 
            \pm \frac{1}{6}h^3f^{(3)}(x) + O(h^4)\\
  f(x\pm 2h) &= f(x) \pm 2hf'(x) +2h^2f''(x)
            \pm \frac{4}{3}h^3f^{(3)}(x) + O(h^4)
\end{align*}
Bestemmer $D_1 = f(x+h) - f(x-h)$ og $D_2 = f(x+2h) - f(x-2h)$ (Op til $O(h^4)$).
\begin{align*}
  D_1 &= 2hf'(x) + \frac{1}{3}h^3f^{(3)}(x) + O(h^4)\\
  D_2 &= 4hf'(x) + \frac{8}{3}h^3f^{(3)}(x) + O(h^4)
\end{align*}
For at eliminere leddene med $h^3$, bestemmes $8D_1-D_2$ og $f'(x)$ isoleres.
\begin{align*}
  8D_1 - D_2 &= 12hf'(x) + O(h^4)\\
  &\Updownarrow\\
  f'(x) &= \frac{8D_1 - D_2}{12h} - O(h^3)\\[2mm]
  &= \frac{8( f(x+h) - f(x-h) ) - ( f(x+2h) - f(x-2h) )}{12h} - O(h^3)
\end{align*}

***
2. Analyse this method in comparison with FD and CD (theoretically and practically on specific examples).

***

Idet vi er interesseret i $h\to 0$ gælder der specielt for $h<1$ at, $$h>h^2>h^3>\cdots$$. 


Trunkeringsfejlen er givet ved
$$c_1h + c_2h^2 + \cdots$$
Fejlen angives som store-O af $h$ i den laveste potens, idet $h<1$.

For at sammenligne metoderne, betragtes deres trunkeringsfejl. 

- FD er $O(h)$
- CD er $O(h^2)$
- Udvidet CD er $O(h^3)$

Forward difference
\begin{align*}
\frac{\partial f}{\partial x_i}(x)\approx\frac{f(x+\varepsilon e_i) - f(x)}{\varepsilon e_i}, \quad \text{for }i=1,\ldots,n.
\end{align*}

Central difference
\begin{align*}
\frac{\partial f}{\partial x_i}(x)\approx\frac{f(x+\varepsilon e_i) - f(x - \varepsilon e_i)}{\varepsilon e_i}, \quad \text{for }i=1,\ldots,n.
\end{align*}

Udvidet central difference
\begin{align*}
\frac{\partial f}{\partial x_i}(x)\approx\frac{8( f(x+\varepsilon e_i) - f(x-\varepsilon e_i) ) - ( f(x+2\varepsilon e_i) - f(x-2\varepsilon e_i) )}{12\varepsilon e_i}, \quad\text{for }i=1,\ldots,n
\end{align*}
Det ses at vores metode har den laveste/bedste trunkeringsfejl. Dette kommer på bekostning af ekstra funktionsevalueringer.

De forskellige metoder kræver forskellinge mængder funktionsevalueringer

- FD kræver $n+1$ funktionsevalueringer
- CD kræver $2n$ funktionsevalueringer
- Udvidet CD kræver $4n$ funktionsevalueringer


```{r}
eps <- .Machine$double.eps
x <- pi/3

f        <- function(x){cos(cos(x)*sin(x))}
df.exact <- function(x){-cos(2*x)*sin( cos(x)*sin(x) )}
FD       <- function(x,h){(f(x+h) - f(x))/h}
CD       <- function(x,h){(f(x+h) - f(x-h))/(2*h)}
ExpCD    <- function(x,h){(8*( f(x+h) - f(x-h) )
                          - ( f(x+2*h) - f(x-2*h) ))/(12*h)}

FDPlot <- CDPlot <- ExpCDPlot <- h <- numeric(16)
for (i in 1:16){
h[i] <- 10^{-i}
FDPlot[i]    <- FD(x,h[i])
CDPlot[i]    <- CD(x,h[i])
ExpCDPlot[i] <- ExpCD(x,h[i])
}


plot(log10(h) , log10( abs(ExpCDPlot - df.exact(x)) ) , 
     type = "l" , col = "green" , 
     xlab = "log10(h)" , ylab="log10(Afvigelse)")
lines(log10(h) , log10( abs(CDPlot - df.exact(x)) ) , 
      col = "blue")

lines(log10(h) , log10( abs(FDPlot - df.exact(x)) ) , 
      col = "red")
legend(-16, -9.5, legend=c("FD", "CD" , "ExpCD"),
       col=c("red", "blue" , "green" ), lty=1, cex=0.8)

```

Det ses at for tilpas store $h$ (ca. $h=10^{-6}$), vil den udvidede CD være et bedre approksimat af $f'(x)$ end CD og FD, idet afvigelsen for disse værdier af $h$, er lavere end CD og FD (se grøn kurve). For dette eksempel (Dim = 1) ses det at ExpCD kræver 4 funktionsevalueringer, hvilket er dobbelt så mange som for både FD og CD. 

***
3. What are the advantages and disadvantages of the different finite difference methods?

***
(Denne opgave er skrevet ind i opgave 1.2)


# Exercise 2

Implement algorithmic differentiation (AD) for univariate ($\mathbb{R} \to \mathbb{R}$) functions in `R` (supporting the following operations: `+`, `-`, `*`, `/`, `sin`, `cos`, `exp`). Use on the following problem and compare it with other ways of calculating the derivatives:

\[
  f(x) = \cos[ \sin(x) \cos(x) ]
\]

```{r}
# Definerer hvad et "ADnum" er.
create_ADnum = function(val, deriv = 1) {
x = list(val = val, deriv = deriv)
class(x) = "ADnum"
return(x)
}

# Definerer hvad der forstås ved at printe et ADnum
print.ADnum = function(x, ...) {
cat("value = ", x$val,
" and deriv = ", x$deriv, "\n", sep = "")
return(invisible(x))
}
     
#Definerer produktreglen og brøkreglen
Ops.ADnum = function(e1, e2) {
# LHS constant, e.g. 2*x: e1 is a number 2, convert to ADnum
if (.Method[1] == "") {
e1 = create_ADnum(e1, 0)
}

# RHS constant, e.g. x*2: e2 is a number 2, convert to ADnum
if (.Method[2] == "") {
e2 = create_ADnum(e2, 0)
}

# Definerer hvordan man adderer ADnums
if (.Generic == "+") {
return(create_ADnum(e1$val + e2$val, (e1$deriv + e2$deriv)))
}
  
# Definerer hvordan man subtraherer ADnums
if (.Generic == "-") {
return(create_ADnum(e1$val - e2$val, (e1$deriv - e2$deriv)))
}
  
# Definerer hvordan man ganger ADnums (Produktregel) 
if (.Generic == "*") {
return(create_ADnum(e1$val * e2$val, e1$deriv*e2$val + e2$deriv*e1$val))
}

# Definerer hvordan man dividerer ADnums (Brøkregel)  
if (.Generic == "/") {
return(create_ADnum(e1$val / e2$val, (e1$deriv*e2$val - e2$deriv*e1$val)/e2$val^2))
}
if (.Generic == "^") {
return(create_ADnum(e1$value^e2$value, e2$value*e1$value^(e2$value - 1) ))
}
stop("Function ’", .Generic, "’ not yet implemented for ADnum")
}

#Definerer cos, sin og exp
Math.ADnum = function(x, ...) {
if (.Generic == "cos") {
  return(create_ADnum(cos(x$val), -sin(x$val)*x$deriv))
} 
else if (.Generic == "sin") {
  return(create_ADnum(sin(x$val), cos(x$val)*x$deriv))
} 
else if (.Generic == "exp") {
  return(create_ADnum(exp(x$val), exp(x$val)*x$deriv))
}

stop("Function ’", .Generic, "’ not yet implemented for ADnum")
}


x <- create_ADnum(pi/2)

cos(sin(cos(x)))


y <- create_ADnum(cos(x$val), -sin(x$val)*x$deriv)
cos(y)

z <- create_ADnum(sin(y$val), cos(y$val)*y$deriv)
cos(z)

f <- function(x){cos(sin(cos(x)))}
f(x)

```



Extend your implementation to handle multivariate ($\mathbb{R} \to \mathbb{R}$) functions and use on the following problem and compare it with other ways of calculating the derivatives:

\[
  f(x) = [ x_1 x_2 \sin(x_3) + \exp(x_1 x_2) ] / x_3 \tag{8.26}
\]

# Exercise 3

In a gradient descent problem (e.g. Rosenbrock's function or best straight line for `cars` dataset), compare the use of exact and numerical derivatives and discuss it. The comparisons can include e.g. illustrations or summary measures (number of iterations, amount of time spent, accuracy of solution and possibly other aspects).

Remember that in `R`, there are many ways of registering amount of time spent. For very fast operations, you can use:

```{r}
library(microbenchmark)
X <- model.matrix(~ speed, cars)
microbenchmark(lm(dist ~ speed, cars), 
               lm.fit(X, cars$dist),
               lm.fit(model.matrix(~ speed, cars), cars$dist), 
               times = 100)
```

For slower operations, you can do it manually:

```{r}
time_begin <- proc.time()

for (i in 1:1000) {
  lm(dist ~ speed, cars)
}

time_end <- proc.time()
time_duration <- time_end - time_begin
time_duration_secs <- time_duration["user.self"]
time_duration_secs
```


TEST
```{r}
f <- function(x) x^2
g <- function(x) 2*x


steepest_descent <- function(f, g, x_k, alpha, c, rho, tol, k_max) {
  k <- 0
	g_k <- tol + 1 # For at komme i gang
	curve(f, from = -3 , to = 3)
	while ((abs(g_k) > tol) & (k < k_max)) { # Type 2 er for vektorer
		g_k <- g(x_k)
		p_k <- -g_k 
	  alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
		x_old <- x_k
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
		
		lines(c(x_old,f(x_old)) , c(x_k,f(x_k)) , col = "red")
	}
	
	printfln("k = %d \t x_k = %6.3f \t f_k = %.2f \t g_k = %8.3f \t alpha_k = %.5f", k, x_k, f(x_k), g_k, alpha_k)
	
	return(x_k)
}

x_sol <- steepest_descent(f,g,-3,1,1e-4,0.5,1e-4,1000)
```


# Exercise 4: Be creative!

If you have anything, put it here.

