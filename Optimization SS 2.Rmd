---
title: 'Optimisation: Self study 2'
output:
  html_document: default
  pdf_document: default
---
\usepackage{amsmath}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("microbenchmark")
printf <- function(...) cat(sprintf(...))
printfln <- function(...) cat(sprintf(...), "\n")
```

# Exercise 1

We have considered forward-difference (using $f(x)$ and $f(x + h)$) and central-difference (using $f(x - h)$ and $f(x + h)$). 

***
1. What would happen if we extend the central-difference to also use $f(x - 2h)$ and $f(x + 2h)$? Hint: consider the Taylor series up to a sufficiently high power of $h$. Hint: "five-point stencil".

***
Taylors formel er givet ved
\begin{align*}
  f(x) = f(x) + h f'(x) + \frac{1}{2!} h^2 f''(x) + \frac{1}{3!} h^3 f^{(3)}(x) + \cdots.
\end{align*}

Forward difference (FD) er givet ved
\begin{align*}
f'(x)\approx\frac{f(x+h) - f(x)}{h}.
\end{align*}
Kaldes Backward difference hvis første led i tælleren er $f(x-h)$.

Central difference (CD) er givet ved
\begin{align*}
f'(x)\approx\frac{f(x+h) - f(x-h)}{2h}.
\end{align*}

Taylors formel benyttes til at approximere funktionerne $f(x\pm h)$ og $f(x\pm 2h)$.
\begin{align*}
  f(x\pm h)  &= f(x) \pm hf'(x) + \frac{1}{2}h^2f''(x) 
            \pm \frac{1}{6}h^3f^{(3)}(x) + O(h^4)\\
  f(x\pm 2h) &= f(x) \pm 2hf'(x) +2h^2f''(x)
            \pm \frac{4}{3}h^3f^{(3)}(x) + O(h^4)
\end{align*}
Bestemmer $D_1 = f(x+h) - f(x-h)$ og $D_2 = f(x+2h) - f(x-2h)$.
\begin{align*}
  D_1 &= 2hf'(x) + \frac{1}{3}h^3f^{(3)}(x) + O(h^4)\\
  D_2 &= 4hf'(x) + \frac{8}{3}h^3f^{(3)}(x) + O(h^4)
\end{align*}
For at eliminere leddene med $h^3$, bestemmes $8D_1-D_2$ og $f'(x)$ isoleres.
\begin{align*}
  8D_1 - D_2 &= 12hf'(x) + O(h^4)\\
  &\Updownarrow\\
  f'(x) &= \frac{8D_1 - D_2}{12h} - O(h^3)\\[2mm]
  &= \frac{8( f(x+h) - f(x-h) ) - ( f(x+2h) - f(x-2h) )}{12h} - O(h^3)
\end{align*}

***
2. Analyse this method in comparison with FD and CD (theoretically and practically on specific examples).

***

For at sammenligne de forskellige metoder, betragtes de tilhørende trunkeringsfejl.

For at bestemme trunkleringsfejlen for \textit{forward difference} betragtes Taylorudvidelsen hvor $f'(x)$ er isoleret:
\begin{align*}
f'(x)=\frac{f(x+h) - f(x)}{h} - \frac{1}{2} h f''(x) - \frac{1}{6} h^2 f^{(3)}(x) - O(h^3).
\end{align*}

Trunkeringsfejlen som en funktion af $h$ er givet ved
$$c_1 h + c_2 h^2 + \cdots.$$ 
Idet vi er interesseret i $h \to 0$. Specielt gælder der for $h < 1$ at, $$h > h^2 > h^3 > \cdots.$$
Dette betyder at trunkeringsfejlen er givet ved
\begin{align*}
    O(h)
\end{align*}

Trunkeringsfejlene for de forskellige metoder er givet ved. 

- FD er $O(h)$
- CD er $O(h^2)$
- Udvidet CD er $O(h^3)$

For at sammenligne antal funktionsevalueringer opskrives de forskellige metoder for $x \in \mathbb R^n$.

Forward difference:
\begin{align*}
\frac{\partial f}{\partial x_i}(x)\approx\frac{f(x +h e_i) - f(x)}{h e_i}, \quad \text{for }i=1,\ldots,n.
\end{align*}

Central difference:
\begin{align*}
\frac{\partial f}{\partial x_i}(x)\approx\frac{f(x + h e_i) - f(x - h e_i)}{h e_i}, \quad \text{for }i=1,\ldots,n.
\end{align*}

Udvidet central difference:
\begin{align*}
\frac{\partial f}{\partial x_i}(x)\approx\frac{8( f(x + h e_i) - f(x- h e_i) ) - ( f(x+2 h e_i) - f(x-2 h e_i) )}{12 h e_i}, \quad\text{for }i=1,\ldots,n
\end{align*}

Metoderne kræver følgende antal funktionsevalueringer

- FD kræver $n+1$ funktionsevalueringer
- CD kræver $2n$ funktionsevalueringer
- Udvidet CD kræver $4n$ funktionsevalueringer


**Praktisk eksempel for sammenligning af metoderne**

```{r}
eps <- .Machine$double.eps
x <- pi/3

f        <- function(x){cos(cos(x)*sin(x))}
df.exact <- function(x){-cos(2*x)*sin( cos(x)*sin(x) )}
FD       <- function(x,h){(f(x+h) - f(x))/h}
CD       <- function(x,h){(f(x+h) - f(x-h))/(2*h)}
ExpCD    <- function(x,h){(8*( f(x+h) - f(x-h) )
                          - ( f(x+2*h) - f(x-2*h) ))/(12*h)}

FD_val <- CD_val <- ExpCD_val <- h <- numeric(16)
for (i in seq(1,16, length.out = 16)){
h[i] <- 10^{-i}
FD_val[i]    <- FD(x,h[i])
CD_val[i]    <- CD(x,h[i])
ExpCD_val[i] <- ExpCD(x,h[i])
}

FD_err    <- abs(FD_val - df.exact(x))
CD_err    <- abs(CD_val - df.exact(x))
ExpCD_err <- abs(ExpCD_val - df.exact(x))

plot(log10(h) , log10( ExpCD_err ) , 
     type = "l" , col = "green" , xlab = "log10(h)" , ylab="log10(Afvigelse)")
lines(log10(h) , log10( CD_err ) , col = "blue")
lines(log10(h) , log10( FD_err ) , col = "red")

legend(-16, -9.5, legend=c("FD", "CD" , "ExpCD"),
       col=c("red", "blue" , "green" ), lty=1, cex=0.8)

```

Det ses at for tilpas store $h$ (ca. $h=10^{-6}$), vil den udvidede CD være et bedre approksimat af $f'(x)$ end CD og FD, idet afvigelsen for disse værdier af $h$, er lavere end CD og FD (se grøn kurve). For dette eksempel (Dim = 1) ses det at ExpCD kræver 4 funktionsevalueringer, hvilket er dobbelt så mange som for både FD og CD. 


***
3. What are the advantages and disadvantages of the different finite difference methods?
Testtekst
***
(Denne opgave er skrevet ind i opgave 1.2)


# Exercise 2

Implement algorithmic differentiation (AD) for univariate ($\mathbb{R} \to \mathbb{R}$) functions in `R` (supporting the following operations: `+`, `-`, `*`, `/`, `sin`, `cos`, `exp`). Use on the following problem and compare it with other ways of calculating the derivatives:

\[
  f(x) = \cos[ \sin(x) \cos(x) ]
\]

```{r}
# Definerer hvad et "ADnum" er.
create_ADnum = function(val, deriv = 1) {
x = list(val = val, deriv = deriv)
class(x) = "ADnum"
return(x)
}

# Definerer hvad der forstås ved at printe et ADnum
print.ADnum = function(x, ...) {
cat("value = ", x$val,
" and deriv = ", x$deriv, "\n", sep = "")
return(invisible(x))
}
     
#Definerer produktreglen og brøkreglen
Ops.ADnum = function(e1, e2) {
# LHS constant, e.g. 2*x: e1 is a number 2, convert to ADnum
if (.Method[1] == "") {
e1 = create_ADnum(e1, 0)
}

# RHS constant, e.g. x*2: e2 is a number 2, convert to ADnum
if (.Method[2] == "") {
e2 = create_ADnum(e2, 0)
}

# Definerer hvordan man adderer ADnums
if (.Generic == "+") {
return(create_ADnum(e1$val + e2$val, (e1$deriv + e2$deriv)))
}
  
# Definerer hvordan man subtraherer ADnums
if (.Generic == "-") {
return(create_ADnum(e1$val - e2$val, (e1$deriv - e2$deriv)))
}
  
# Definerer hvordan man ganger ADnums (Produktregel) 
if (.Generic == "*") {
return(create_ADnum(e1$val * e2$val, e1$deriv*e2$val + e2$deriv*e1$val))
}

# Definerer hvordan man dividerer ADnums (Brøkregel)  
if (.Generic == "/") {
return(create_ADnum(e1$val / e2$val, (e1$deriv*e2$val - e2$deriv*e1$val)/e2$val^2))
}
if (.Generic == "^") {
return(create_ADnum(e1$value^e2$value, e2$value*e1$value^(e2$value - 1) ))
}
stop("Function ’", .Generic, "’ not yet implemented for ADnum")
}

#Definerer cos, sin og exp
Math.ADnum = function(x, ...) {
if (.Generic == "cos") {
  return(create_ADnum(cos(x$val), -sin(x$val)*x$deriv))
} 
else if (.Generic == "sin") {
  return(create_ADnum(sin(x$val), cos(x$val)*x$deriv))
} 
else if (.Generic == "exp") {
  return(create_ADnum(exp(x$val), exp(x$val)*x$deriv))
}

stop("Function ’", .Generic, "’ not yet implemented for ADnum")
}


x <- create_ADnum(pi/2)

cos(sin(cos(x)))


y <- create_ADnum(cos(x$val), -sin(x$val)*x$deriv)
cos(y)

z <- create_ADnum(sin(y$val), cos(y$val)*y$deriv)
cos(z)

f <- function(x){cos(sin(cos(x)))}
f(x)

```



Extend your implementation to handle multivariate ($\mathbb{R} \to \mathbb{R}$) functions and use on the following problem and compare it with other ways of calculating the derivatives:

\[
  f(x) = [ x_1 x_2 \sin(x_3) + \exp(x_1 x_2) ] / x_3 \tag{8.26}
\]

# Exercise 3

In a gradient descent problem (e.g. Rosenbrock's function or best straight line for `cars` dataset), compare the use of exact and numerical derivatives and discuss it. The comparisons can include e.g. illustrations or summary measures (number of iterations, amount of time spent, accuracy of solution and possibly other aspects).

Remember that in `R`, there are many ways of registering amount of time spent. For very fast operations, you can use:

```{r}
library(microbenchmark)
X <- model.matrix(~ speed, cars)
microbenchmark(lm(dist ~ speed, cars), 
               lm.fit(X, cars$dist),
               lm.fit(model.matrix(~ speed, cars), cars$dist), 
               times = 100)
```

For slower operations, you can do it manually:

```{r}
time_begin <- proc.time()

for (i in 1:1000) {
  lm(dist ~ speed, cars)
}

time_end <- proc.time()
time_duration <- time_end - time_begin
time_duration_secs <- time_duration["user.self"]
time_duration_secs
```


TEST
```{r}
f <- function(x) x^2
g <- function(x) 2*x

backtracking_line_search <- function(alpha, c, rho, x_k, p_k, g_k, f) {
	alpha_k <- alpha
	repeat { 
		lhs <- f(x_k + alpha_k * p_k)
		rhs <- f(x_k) + c * alpha_k * g_k %*% p_k
		if (lhs <= rhs) break
		alpha_k <- rho * alpha_k
	}
	return(alpha_k)
}

steepest_descent <- function(f, g, x_k, alpha, c, rho, tol, k_max) {
  k <- 0
	g_k <- tol + 1 # For at komme i gang
	curve(f, from = -4 , to = 4)
	while ((abs(g_k) > tol) & (k < k_max)) { # Type 2 er for vektorer
		g_k <- g(x_k)
		p_k <- -g_k 
	  alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
		x_old <- x_k
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
		
		lines(c(x_old,x_k) , c(f(x_old),f(x_k)) , col = "red")
	}
	
	printfln("k = %d \t x_k = %6.3f \t f_k = %.2f \t g_k = %8.3f \t alpha_k = %.5f", k, x_k, f(x_k), g_k, alpha_k)
	
	return(x_k)
}
steepest_descent_CD <- function(f, x_k, alpha, c, rho, tol, k_max) {
  k <- 0
	g_k <- tol + 1 # For at komme i gang
	curve(f, from = -4 , to = 4)
	while ((abs(g_k) > tol) & (k < k_max)) { # Type 2 er for vektorer
		g_k <- CD(x_k,1e-04)
		p_k <- -g_k 
	  alpha_k <- backtracking_line_search(alpha, c, rho, x_k, p_k, g_k, f)
		x_old <- x_k
		x_k <- x_k + alpha_k * p_k
		k <- k + 1
		
		lines(c(x_old,x_k) , c(f(x_old),f(x_k)) , col = "red")
	}
	
	printfln("k = %d \t x_k = %6.3f \t f_k = %.2f \t g_k = %8.3f \t alpha_k = %.5f", k, x_k, f(x_k), g_k, alpha_k)
	
	return(x_k)
}


x_sol   <- steepest_descent(f,g,-2,0.9,1e-4,0.5,1e-4,1000)
x_solCD <- steepest_descent_CD(f,-2,0.9,1e-4,0.5,1e-4,1000)
```


# Exercise 4: Be creative!

If you have anything, put it here.

